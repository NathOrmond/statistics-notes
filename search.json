[
  {
    "objectID": "concepts/template.html",
    "href": "concepts/template.html",
    "title": "Concept Name",
    "section": "",
    "text": "Brief introduction to the statistical concept and why it’s important."
  },
  {
    "objectID": "concepts/template.html#introduction",
    "href": "concepts/template.html#introduction",
    "title": "Concept Name",
    "section": "",
    "text": "Brief introduction to the statistical concept and why it’s important."
  },
  {
    "objectID": "concepts/template.html#key-learning-objectives",
    "href": "concepts/template.html#key-learning-objectives",
    "title": "Concept Name",
    "section": "Key Learning Objectives",
    "text": "Key Learning Objectives\n\nObjective 1\nObjective 2\nObjective 3"
  },
  {
    "objectID": "concepts/template.html#step-1-first-step",
    "href": "concepts/template.html#step-1-first-step",
    "title": "Concept Name",
    "section": "Step 1: [First Step]",
    "text": "Step 1: [First Step]\nDescription of the first step.\n\n\nShow the code\n# Check and install required packages\nrequired_packages &lt;- c(\"ggplot2\", \"dplyr\")\n\n# Load required packages\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n# Install and load packages\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Your code here"
  },
  {
    "objectID": "concepts/template.html#step-2-second-step",
    "href": "concepts/template.html#step-2-second-step",
    "title": "Concept Name",
    "section": "Step 2: [Second Step]",
    "text": "Step 2: [Second Step]\nDescription of the second step.\n\n\nShow the code\n# Your code here"
  },
  {
    "objectID": "concepts/template.html#step-3-third-step",
    "href": "concepts/template.html#step-3-third-step",
    "title": "Concept Name",
    "section": "Step 3: [Third Step]",
    "text": "Step 3: [Third Step]\nDescription of the third step.\n\n\nShow the code\n# Your code here"
  },
  {
    "objectID": "concepts/template.html#key-insights",
    "href": "concepts/template.html#key-insights",
    "title": "Concept Name",
    "section": "Key Insights",
    "text": "Key Insights\n\nInsight 1: Description\nInsight 2: Description\nInsight 3: Description"
  },
  {
    "objectID": "concepts/template.html#mathematical-foundation",
    "href": "concepts/template.html#mathematical-foundation",
    "title": "Concept Name",
    "section": "Mathematical Foundation",
    "text": "Mathematical Foundation\nMathematical notation and formulas related to the concept."
  },
  {
    "objectID": "concepts/template.html#further-reading",
    "href": "concepts/template.html#further-reading",
    "title": "Concept Name",
    "section": "Further Reading",
    "text": "Further Reading\n\nReference 1\nReference 2\nReference 3"
  },
  {
    "objectID": "concepts/central-limit-theorem.html",
    "href": "concepts/central-limit-theorem.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Show the code\n# Installation and Setup\nrequired_packages &lt;- c(\"ggplot2\", \"dplyr\", \"gridExtra\", \"moments\", \"nortest\")\n\n# Load required packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\nlibrary(moments)\nlibrary(nortest)\n\nset.seed(123)\nShow the code\ngenerate_distribution &lt;- function(dist_type, n = 10000, params = list()) {\n  switch(dist_type,\n         \"exponential\" = rexp(n, rate = params$rate %||% 0.5),\n         \"normal\" = rnorm(n, mean = params$mean %||% 0, sd = params$sd %||% 1),\n         \"uniform\" = runif(n, min = params$min %||% 0, max = params$max %||% 10),\n         \"gamma\" = rgamma(n, shape = params$shape %||% 2, rate = params$rate %||% 0.5),\n         \"beta\" = rbeta(n, shape1 = params$shape1 %||% 2, shape2 = params$shape2 %||% 5),\n         \"lognormal\" = rlnorm(n, meanlog = params$meanlog %||% 0, sdlog = params$sdlog %||% 1),\n         \"chi_squared\" = rchisq(n, df = params$df %||% 3),\n         \"weibull\" = rweibull(n, shape = params$shape %||% 2, scale = params$scale %||% 1),\n         rexp(n, rate = 0.5)  # default to exponential\n  )\n}\n# Helper function for default parameters\n`%||%` &lt;- function(x, y) if (is.null(x)) y else x\n# Global parameters\nn_population &lt;- 10000\nsample_size &lt;- 30  # Size of each sample\nn_samples &lt;- 1000  # Number of samples to take"
  },
  {
    "objectID": "concepts/central-limit-theorem.html#introduction",
    "href": "concepts/central-limit-theorem.html#introduction",
    "title": "Central Limit Theorem",
    "section": "Introduction",
    "text": "Introduction\nThe Central Limit Theorem (CLT) is one of the most important concepts in statistics. It states that when we take repeated random samples from any population (regardless of the shape of the original distribution), the sampling distribution of the sample mean will approach a normal distribution as the sample size increases."
  },
  {
    "objectID": "concepts/central-limit-theorem.html#simulation-1-relating-population-distributions-to-sample-distributions",
    "href": "concepts/central-limit-theorem.html#simulation-1-relating-population-distributions-to-sample-distributions",
    "title": "Central Limit Theorem",
    "section": "Simulation 1: Relating Population Distributions to Sample Distributions",
    "text": "Simulation 1: Relating Population Distributions to Sample Distributions\n\n\nShow the code\n# Distributions\n# \"exponential\", \"normal\", \"uniform\", \"gamma\", \"beta\", \"lognormal\", \"chi_squared\", \"weibull\"\noriginal_distribution &lt;- generate_distribution(\"exponential\")  \n\n\n\n\nShow the code\n# Create distribution with default parameters\noriginal_distribution &lt;- generate_distribution(\"exponential\")\n# Create a data frame for plotting\ndf_original &lt;- data.frame(values = original_distribution)\n# Plot the original distribution\nggplot(df_original, aes(x = values)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7, color = \"black\") +\n  geom_density(aes(y = after_stat(count) * max(after_stat(count)) / max(after_stat(density))), \n               color = \"red\", linewidth = 1) +\n  labs(title = \"Original Distribution (Exponential)\",\n       subtitle = paste(\"Mean =\", round(mean(original_distribution), 2), \n                       \", SD =\", round(sd(original_distribution), 2)),\n       x = \"Values\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 1: Generate random samples from the distribution\nsample_means &lt;- numeric(n_samples)\nsample_sds &lt;- numeric(n_samples)\nsample_data_list &lt;- list()\nfor(i in 1:n_samples) {\n  sample_data &lt;- sample(original_distribution, size = sample_size, replace = TRUE)\n  sample_means[i] &lt;- mean(sample_data)\n  sample_sds[i] &lt;- sd(sample_data)\n  sample_data_list[[i]] &lt;- sample_data\n}\ncat(\"Generated\", n_samples, \"samples of size\", sample_size, \"\\n\")\n\n\nGenerated 1000 samples of size 30 \n\n\nShow the code\ncat(\"First few sample means:\", round(sample_means[1:5], 3), \"\\n\")\n\n\nFirst few sample means: 1.691 2.024 2.07 1.544 2.131 \n\n\n\n\nShow the code\n# Visualize where samples are drawn from the population\n# Show first 3 samples as points on the population distribution\nsample_points &lt;- data.frame()\nfor(i in 1:3) {\n  sample_data &lt;- sample_data_list[[i]]\n  sample_points &lt;- rbind(sample_points, \n                        data.frame(values = sample_data, \n                                  sample = paste(\"Sample\", i),\n                                  mean = rep(mean(sample_data), length(sample_data))))\n}\n\n# Plot population with sample points overlaid\nggplot() +\n  geom_histogram(data = df_original, aes(x = values), \n                bins = 50, fill = \"steelblue\", alpha = 0.7, color = \"black\") +\n  geom_density(data = df_original, aes(x = values, y = after_stat(count) * max(after_stat(count)) / max(after_stat(density))), \n               color = \"red\", linewidth = 1) +\n  geom_point(data = sample_points, aes(x = values, y = 0, color = sample), \n             size = 2, alpha = 0.8, position = position_jitter(height = 50)) +\n  geom_vline(data = sample_points, aes(xintercept = mean, color = sample), \n             linewidth = 1, linetype = \"dashed\") +\n  labs(title = \"Population Distribution with Sample Points\",\n       subtitle = \"Points show individual values from first 3 samples\\nDashed lines show sample means\",\n       x = \"Values\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_color_manual(values = c(\"Sample 1\" = \"orange\", \"Sample 2\" = \"purple\", \"Sample 3\" = \"green\"))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 2: Create data frame for the sampling distribution\ndf_sample_means &lt;- data.frame(sample_mean = sample_means)\ncat(\"Created data frame with\", nrow(df_sample_means), \"sample means\\n\")\n\n\nCreated data frame with 1000 sample means\n\n\nImportant Note: The samples are drawn from the original population distribution (exponential in this case). What you’re seeing is the sampling distribution of the means, which is different from the original population distribution. This is exactly what the Central Limit Theorem predicts - even though we’re sampling from a skewed exponential distribution, the distribution of sample means becomes approximately normal!\n\n\nShow the code\n# Step 3a: Calculate key statistics for the sampling distribution\nsampling_mean &lt;- mean(sample_means)\nsampling_sd &lt;- sd(sample_means)\npopulation_mean &lt;- mean(original_distribution)\ntheoretical_se &lt;- sd(original_distribution) / sqrt(sample_size)\n\ncat(\"=== Sampling Distribution Statistics ===\\n\")\n\n\n=== Sampling Distribution Statistics ===\n\n\nShow the code\ncat(\"Mean of sample means:\", round(sampling_mean, 3), \"\\n\")\n\n\nMean of sample means: 2.024 \n\n\nShow the code\ncat(\"SD of sample means (observed SE):\", round(sampling_sd, 3), \"\\n\")\n\n\nSD of sample means (observed SE): 0.366 \n\n\nShow the code\ncat(\"Population mean:\", round(population_mean, 3), \"\\n\")\n\n\nPopulation mean: 2.006 \n\n\nShow the code\ncat(\"Theoretical standard error:\", round(theoretical_se, 3), \"\\n\")\n\n\nTheoretical standard error: 0.365 \n\n\nShow the code\ncat(\"Difference (observed - theoretical):\", round(sampling_sd - theoretical_se, 3), \"\\n\")\n\n\nDifference (observed - theoretical): 0.001 \n\n\n\n\nShow the code\n# Step 3b: Create the histogram of sample means\nhist_plot &lt;- ggplot(df_sample_means, aes(x = sample_mean)) +\n  geom_histogram(bins = 30, fill = \"lightgreen\", alpha = 0.7, color = \"black\") +\n  labs(title = \"Histogram of Sample Means\",\n       subtitle = paste(\"1000 samples of size\", sample_size),\n       x = \"Sample Mean\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\nprint(hist_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 3c: Add density curve to show the shape\n# Calculate density separately to get proper scaling\ndensity_data &lt;- density(df_sample_means$sample_mean, adjust = 1.5)\n# Scale density to match histogram scale\nmax_freq &lt;- max(hist(df_sample_means$sample_mean, breaks = 30, plot = FALSE)$counts)\ndensity_scaled &lt;- density_data$y * max_freq / max(density_data$y)\n\ndensity_plot &lt;- ggplot(df_sample_means, aes(x = sample_mean)) +\n  geom_histogram(bins = 30, fill = \"lightgreen\", alpha = 0.7, color = \"black\") +\n  geom_line(data = data.frame(x = density_data$x, y = density_scaled), \n            aes(x = x, y = y), color = \"blue\", linewidth = 2) +\n  labs(title = \"Sample Means with Density Curve\",\n       subtitle = \"Blue curve shows the estimated probability density (scaled to match histogram)\",\n       x = \"Sample Mean\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\nprint(density_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 3d: Add population mean reference line\nfinal_plot &lt;- ggplot(df_sample_means, aes(x = sample_mean)) +\n  geom_histogram(bins = 30, fill = \"lightgreen\", alpha = 0.7, color = \"black\") +\n  geom_line(data = data.frame(x = density_data$x, y = density_scaled), \n            aes(x = x, y = y), color = \"blue\", linewidth = 2) +\n  geom_vline(xintercept = population_mean, color = \"red\", linewidth = 2) +\n  labs(title = \"Distribution of Sample Means\",\n       subtitle = paste(\"Mean of means =\", round(sampling_mean, 3), \n                       \", SD of means =\", round(sampling_sd, 3)),\n       x = \"Sample Mean\", y = \"Frequency\",\n       caption = \"Red line = Population mean | Blue curve = Density estimate | Green bars = Histogram of sample means\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        plot.caption = element_text(hjust = 0.5, size = 10))\n\nprint(final_plot)\n\n\n\n\n\n\n\n\n\nWhat Each Step Shows:\n\nStep 3a: Calculates and displays the key statistics, showing how close our observed standard error is to the theoretical prediction\nStep 3b: Shows just the histogram to see the basic shape of the sampling distribution\nStep 3c: Adds the density curve to better visualize the smooth shape and assess normality\nStep 3d: Adds the population mean reference line to show that sample means cluster around the true population mean\n\nNote: These statistics are shown after the graph because they provide a detailed breakdown of what we just visualized. The graph gives us the big picture, while these statistics give us the specific numbers for the first few samples.\n\n\nShow the code\n# Step 4: Show statistics for first few individual samples\ncat(\"=== Individual Sample Statistics ===\\n\")\n\n\n=== Individual Sample Statistics ===\n\n\nShow the code\nfor(i in 1:5) {\n  cat(\"Sample\", i, \"mean:\", round(sample_means[i], 3), \n      \"SD:\", round(sample_sds[i], 3), \"\\n\")\n}\n\n\nSample 1 mean: 1.691 SD: 1.867 \nSample 2 mean: 2.024 SD: 1.821 \nSample 3 mean: 2.07 SD: 1.877 \nSample 4 mean: 1.544 SD: 1.541 \nSample 5 mean: 2.131 SD: 3.022 \n\n\n\n\nShow the code\n# Step 5: Compare population vs sampling distribution\ncat(\"\\n=== Population vs Sampling Distribution ===\\n\")\n\n\n\n=== Population vs Sampling Distribution ===\n\n\nShow the code\ncat(\"Average of all sample means:\", round(mean(sample_means), 3), \"\\n\")\n\n\nAverage of all sample means: 2.024 \n\n\nShow the code\ncat(\"Population mean:\", round(mean(original_distribution), 3), \"\\n\")\n\n\nPopulation mean: 2.006 \n\n\nShow the code\ncat(\"Theoretical SE:\", round(sd(original_distribution)/sqrt(sample_size), 3), \"\\n\")\n\n\nTheoretical SE: 0.365 \n\n\nShow the code\ncat(\"Observed SE:\", round(sd(sample_means), 3), \"\\n\")\n\n\nObserved SE: 0.366 \n\n\n\n\nShow the code\n# Step 6: Load gridExtra for multiple plots\nlibrary(gridExtra)\ncat(\"Loaded gridExtra package for plot arrangement\\n\")\n\n\nLoaded gridExtra package for plot arrangement\n\n\n\n\nShow the code\n# Step 7: Create QQ plots for first 4 individual samples\nqq_plots &lt;- list()\nfor(i in 1:4) {\n  sample_data &lt;- sample_data_list[[i]]\n  \n  p &lt;- ggplot(data.frame(sample_data = sample_data), aes(sample = sample_data)) +\n    stat_qq() +\n    stat_qq_line(color = \"red\", linewidth = 1) +\n    labs(title = paste(\"Sample\", i, \"QQ Plot\"),\n         subtitle = paste(\"Mean =\", round(mean(sample_data), 2), \n                         \", SD =\", round(sd(sample_data), 2)),\n         x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5, size = 10),\n          plot.subtitle = element_text(hjust = 0.5, size = 8))\n  \n  qq_plots[[i]] &lt;- p\n}\ndo.call(grid.arrange, c(qq_plots, ncol = 2))\n\n\n\n\n\n\n\n\n\nUnderstanding the QQ Plots: These QQ plots show how well each individual sample follows a normal distribution. Since we’re sampling from an exponential distribution (which is skewed), individual samples will also be skewed and won’t follow the red line perfectly. This is expected - the CLT applies to the distribution of means, not individual samples.\n\n\nShow the code\nggplot(df_sample_means, aes(sample = sample_mean)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\", linewidth = 1) +\n  labs(title = \"QQ Plot: Distribution of Sample Means\",\n       subtitle = paste(\"Sample size =\", sample_size, \", Number of samples =\", n_samples),\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))"
  },
  {
    "objectID": "concepts/central-limit-theorem.html#simulation-2-adding-together-random-variables-produces-gaussians",
    "href": "concepts/central-limit-theorem.html#simulation-2-adding-together-random-variables-produces-gaussians",
    "title": "Central Limit Theorem",
    "section": "Simulation 2: Adding Together Random Variables Produces Gaussians",
    "text": "Simulation 2: Adding Together Random Variables Produces Gaussians\nThe Central Limit Theorem also explains why adding many independent random variables produces a normal distribution, regardless of the original distributions’ shapes.\n\n\nShow the code\n# Step 1: Set up parameters for the simulation\nset.seed(456)  # Different seed for this simulation\nn_observations &lt;- 10000\nmax_variables &lt;- 10\n\n# Create a skewed distribution (exponential) to start with\nbase_distribution &lt;- rexp(n_observations, rate = 0.5)\ncat(\"Base distribution: Exponential with rate = 0.5\\n\")\n\n\nBase distribution: Exponential with rate = 0.5\n\n\nShow the code\ncat(\"Mean =\", round(mean(base_distribution), 3), \", SD =\", round(sd(base_distribution), 3), \"\\n\")\n\n\nMean = 1.969 , SD = 1.942 \n\n\n\n\nShow the code\n# Step 2: Create data frame for different numbers of variables\nsums_data &lt;- data.frame()\nfor(n_vars in 1:max_variables) {\n  # Generate n_vars independent random variables\n  variables_matrix &lt;- matrix(rexp(n_observations * n_vars, rate = 0.5), \n                           nrow = n_observations, ncol = n_vars)\n  \n  # Sum the variables for each observation\n  sums &lt;- rowSums(variables_matrix)\n  \n  # Add to data frame\n  sums_data &lt;- rbind(sums_data, \n                    data.frame(sum = sums, \n                              n_variables = rep(n_vars, n_observations)))\n}\ncat(\"Generated sums for 1 to\", max_variables, \"random variables\\n\")\n\n\nGenerated sums for 1 to 10 random variables\n\n\n\n\nShow the code\n# Step 3: Plot individual exponential distribution (n=1) with proper density scaling\nsingle_subset &lt;- sums_data[sums_data$n_variables == 1, ]\n# Calculate density with proper scaling\ndensity_single &lt;- density(single_subset$sum, adjust = 1.5)\nmax_freq_single &lt;- max(hist(single_subset$sum, breaks = 50, plot = FALSE)$counts)\ndensity_scaled_single &lt;- density_single$y * max_freq_single / max(density_single$y)\n\nsingle_var_plot &lt;- ggplot(single_subset, aes(x = sum)) +\n  geom_histogram(bins = 50, fill = \"orange\", alpha = 0.7, color = \"black\") +\n  geom_line(data = data.frame(x = density_single$x, y = density_scaled_single), \n            aes(x = x, y = y), color = \"red\", linewidth = 1.5) +\n  labs(title = \"Single Exponential Random Variable\",\n       subtitle = \"Original skewed distribution\",\n       x = \"Value\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\nprint(single_var_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 4: Plot sum of 2 variables with proper density scaling\ntwo_subset &lt;- sums_data[sums_data$n_variables == 2, ]\ndensity_two &lt;- density(two_subset$sum, adjust = 1.5)\nmax_freq_two &lt;- max(hist(two_subset$sum, breaks = 50, plot = FALSE)$counts)\ndensity_scaled_two &lt;- density_two$y * max_freq_two / max(density_two$y)\n\ntwo_var_plot &lt;- ggplot(two_subset, aes(x = sum)) +\n  geom_histogram(bins = 50, fill = \"yellow\", alpha = 0.7, color = \"black\") +\n  geom_line(data = data.frame(x = density_two$x, y = density_scaled_two), \n            aes(x = x, y = y), color = \"red\", linewidth = 1.5) +\n  labs(title = \"Sum of 2 Exponential Random Variables\",\n       subtitle = \"Starting to look more symmetric\",\n       x = \"Sum\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\nprint(two_var_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 5: Plot sum of 5 variables with proper density scaling\nfive_subset &lt;- sums_data[sums_data$n_variables == 5, ]\ndensity_five &lt;- density(five_subset$sum, adjust = 1.5)\nmax_freq_five &lt;- max(hist(five_subset$sum, breaks = 50, plot = FALSE)$counts)\ndensity_scaled_five &lt;- density_five$y * max_freq_five / max(density_five$y)\n\nfive_var_plot &lt;- ggplot(five_subset, aes(x = sum)) +\n  geom_histogram(bins = 50, fill = \"lightgreen\", alpha = 0.7, color = \"black\") +\n  geom_line(data = data.frame(x = density_five$x, y = density_scaled_five), \n            aes(x = x, y = y), color = \"red\", linewidth = 1.5) +\n  labs(title = \"Sum of 5 Exponential Random Variables\",\n       subtitle = \"Much more bell-shaped\",\n       x = \"Sum\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\nprint(five_var_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 6: Plot sum of 10 variables with proper density scaling\nten_subset &lt;- sums_data[sums_data$n_variables == 10, ]\ndensity_ten &lt;- density(ten_subset$sum, adjust = 1.5)\nmax_freq_ten &lt;- max(hist(ten_subset$sum, breaks = 50, plot = FALSE)$counts)\ndensity_scaled_ten &lt;- density_ten$y * max_freq_ten / max(density_ten$y)\n\nten_var_plot &lt;- ggplot(ten_subset, aes(x = sum)) +\n  geom_histogram(bins = 50, fill = \"lightblue\", alpha = 0.7, color = \"black\") +\n  geom_line(data = data.frame(x = density_ten$x, y = density_scaled_ten), \n            aes(x = x, y = y), color = \"red\", linewidth = 1.5) +\n  labs(title = \"Sum of 10 Exponential Random Variables\",\n       subtitle = \"Very close to normal distribution\",\n       x = \"Sum\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\nprint(ten_var_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 7: Compare all distributions side by side\ncomparison_plot &lt;- ggplot(sums_data, aes(x = sum, fill = factor(n_variables))) +\n  geom_density(alpha = 0.6) +\n  labs(title = \"Evolution of Distribution Shape\",\n       subtitle = \"Adding more random variables makes the distribution more normal\",\n       x = \"Sum\", y = \"Density\", fill = \"Number of\\nVariables\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_fill_brewer(palette = \"Set3\")\n\nprint(comparison_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 8: Create QQ plots for each distribution\nqq_plots_list &lt;- list()\n\nfor(n_vars in c(1, 2, 5, 10)) {\n  subset_data &lt;- sums_data[sums_data$n_variables == n_vars, ]\n  \n  p &lt;- ggplot(subset_data, aes(sample = sum)) +\n    stat_qq() +\n    stat_qq_line(color = \"red\", linewidth = 1) +\n    labs(title = paste(\"QQ Plot: Sum of\", n_vars, \"Variables\"),\n         subtitle = paste(\"Mean =\", round(mean(subset_data$sum), 2), \n                         \", SD =\", round(sd(subset_data$sum), 2)),\n         x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5, size = 10),\n          plot.subtitle = element_text(hjust = 0.5, size = 8))\n  \n  qq_plots_list[[paste0(\"n\", n_vars)]] &lt;- p\n}\n\n# Display QQ plots in a grid\ndo.call(grid.arrange, c(qq_plots_list, ncol = 2))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Step 9: Show comprehensive summary statistics\ncat(\"=== Comprehensive Summary Statistics ===\\n\")\n\n\n=== Comprehensive Summary Statistics ===\n\n\nShow the code\ncat(\"Note: Normal distribution has skewness = 0, kurtosis = 3\\n\\n\")\n\n\nNote: Normal distribution has skewness = 0, kurtosis = 3\n\n\nShow the code\nfor(n_vars in c(1, 2, 5, 10)) {\n  subset_data &lt;- sums_data[sums_data$n_variables == n_vars, ]\n  observed_mean &lt;- mean(subset_data$sum)\n  observed_sd &lt;- sd(subset_data$sum)\n  observed_skew &lt;- skewness(subset_data$sum)\n  observed_kurt &lt;- kurtosis(subset_data$sum)\n  \n  # Theoretical values for sum of exponential variables\n  theoretical_mean &lt;- n_vars * 2  # E[X] = 1/rate = 1/0.5 = 2\n  theoretical_sd &lt;- sqrt(n_vars) * 2  # SD = sqrt(n) * (1/rate)\n  \n  cat(\"n =\", n_vars, \"variables:\\n\")\n  cat(\"  Mean: Observed =\", round(observed_mean, 3), \n      \"| Theoretical =\", round(theoretical_mean, 3), \n      \"| Difference =\", round(observed_mean - theoretical_mean, 3), \"\\n\")\n  cat(\"  SD: Observed =\", round(observed_sd, 3), \n      \"| Theoretical =\", round(theoretical_sd, 3), \n      \"| Difference =\", round(observed_sd - theoretical_sd, 3), \"\\n\")\n  cat(\"  Skewness:\", round(observed_skew, 3), \n      \"| Kurtosis:\", round(observed_kurt, 3), \"\\n\")\n  cat(\"  Distance from normal (skewness + |kurtosis-3|):\", \n      round(abs(observed_skew) + abs(observed_kurt - 3), 3), \"\\n\\n\")\n}\n\n\nn = 1 variables:\n  Mean: Observed = 2.03 | Theoretical = 2 | Difference = 0.03 \n  SD: Observed = 2.039 | Theoretical = 2 | Difference = 0.039 \n  Skewness: 2.034 | Kurtosis: 9.162 \n  Distance from normal (skewness + |kurtosis-3|): 8.197 \n\nn = 2 variables:\n  Mean: Observed = 3.975 | Theoretical = 4 | Difference = -0.025 \n  SD: Observed = 2.821 | Theoretical = 2.828 | Difference = -0.007 \n  Skewness: 1.389 | Kurtosis: 5.608 \n  Distance from normal (skewness + |kurtosis-3|): 3.997 \n\nn = 5 variables:\n  Mean: Observed = 10.057 | Theoretical = 10 | Difference = 0.057 \n  SD: Observed = 4.544 | Theoretical = 4.472 | Difference = 0.071 \n  Skewness: 0.915 | Kurtosis: 4.17 \n  Distance from normal (skewness + |kurtosis-3|): 2.084 \n\nn = 10 variables:\n  Mean: Observed = 19.99 | Theoretical = 20 | Difference = -0.01 \n  SD: Observed = 6.325 | Theoretical = 6.325 | Difference = 0.001 \n  Skewness: 0.632 | Kurtosis: 3.657 \n  Distance from normal (skewness + |kurtosis-3|): 1.29 \n\n\n\n\nShow the code\n# Step 10: Systematic analysis of normality convergence\ncat(\"=== Systematic Analysis: When Does It Become 'Normal'? ===\\n\")\n\n\n=== Systematic Analysis: When Does It Become 'Normal'? ===\n\n\nShow the code\n# Calculate normality measures for all n from 1 to 10\nnormality_analysis &lt;- data.frame()\nfor(n_vars in 1:10) {\n  subset_data &lt;- sums_data[sums_data$n_variables == n_vars, ]\n  \n  # Calculate various normality measures\n  skew &lt;- abs(skewness(subset_data$sum))\n  kurt &lt;- abs(kurtosis(subset_data$sum) - 3)\n  distance_from_normal &lt;- skew + kurt\n  \n  # Shapiro-Wilk test p-value (higher = more normal) - only for n &gt;= 3\n  if(nrow(subset_data) &gt;= 3 && nrow(subset_data) &lt;= 5000) {\n    shapiro_p &lt;- shapiro.test(subset_data$sum)$p.value\n  } else {\n    shapiro_p &lt;- NA\n  }\n  \n  # Anderson-Darling test statistic (lower = more normal) - only for n &gt;= 8\n  if(nrow(subset_data) &gt;= 8) {\n    ad_stat &lt;- ad.test(subset_data$sum)$statistic\n  } else {\n    ad_stat &lt;- NA\n  }\n  \n  normality_analysis &lt;- rbind(normality_analysis, \n                            data.frame(n_variables = n_vars,\n                                     skewness = skew,\n                                     kurtosis_deviation = kurt,\n                                     distance_from_normal = distance_from_normal,\n                                     shapiro_p = shapiro_p,\n                                     anderson_darling = ad_stat))\n}\n\n# Print the analysis\nprint(normality_analysis)\n\n\n   n_variables  skewness kurtosis_deviation distance_from_normal shapiro_p\nA            1 2.0343137          6.1624794             8.196793        NA\nA1           2 1.3886897          2.6084032             3.997093        NA\nA2           3 1.2005207          2.1619714             3.362492        NA\nA3           4 0.9995151          1.5278877             2.527403        NA\nA4           5 0.9148454          1.1696152             2.084461        NA\nA5           6 0.8761856          1.2382380             2.114424        NA\nA6           7 0.8055370          1.0913248             1.896862        NA\nA7           8 0.6367657          0.5265843             1.163350        NA\nA8           9 0.6653845          0.6875862             1.352971        NA\nA9          10 0.6321731          0.6573511             1.289524        NA\n   anderson_darling\nA         472.19770\nA1        235.57296\nA2        159.69691\nA3        110.44024\nA4         97.17577\nA5         81.42825\nA6         64.54992\nA7         47.71938\nA8         48.94546\nA9         40.60021\n\n\nShow the code\n# Find when it becomes \"practically normal\"\ncat(\"\\n=== Practical Normality Thresholds ===\\n\")\n\n\n\n=== Practical Normality Thresholds ===\n\n\nShow the code\ncat(\"Common thresholds for 'practically normal':\\n\")\n\n\nCommon thresholds for 'practically normal':\n\n\nShow the code\ncat(\"- |Skewness| &lt; 0.5: n =\", min(which(normality_analysis$skewness &lt; 0.5)), \"\\n\")\n\n\n- |Skewness| &lt; 0.5: n = Inf \n\n\nShow the code\ncat(\"- |Kurtosis-3| &lt; 0.5: n =\", min(which(normality_analysis$kurtosis_deviation &lt; 0.5)), \"\\n\")\n\n\n- |Kurtosis-3| &lt; 0.5: n = Inf \n\n\nShow the code\n# Only show Shapiro-Wilk results if we have valid p-values\nvalid_shapiro &lt;- !is.na(normality_analysis$shapiro_p)\nif(any(valid_shapiro)) {\n  cat(\"- Shapiro-Wilk p &gt; 0.05: n =\", min(which(normality_analysis$shapiro_p &gt; 0.05 & valid_shapiro)), \"\\n\")\n  cat(\"- Shapiro-Wilk p &gt; 0.10: n =\", min(which(normality_analysis$shapiro_p &gt; 0.10 & valid_shapiro)), \"\\n\")\n} else {\n  cat(\"- Shapiro-Wilk test: Not available (sample size constraints)\\n\")\n}\n\n\n- Shapiro-Wilk test: Not available (sample size constraints)\n\n\nShow the code\ncat(\"- Distance from normal &lt; 1.0: n =\", min(which(normality_analysis$distance_from_normal &lt; 1.0)), \"\\n\")\n\n\n- Distance from normal &lt; 1.0: n = Inf \n\n\n\n\nShow the code\n# Step 11: Visualize the convergence to normality\nconvergence_plot &lt;- ggplot(normality_analysis, aes(x = n_variables)) +\n  geom_line(aes(y = skewness, color = \"Skewness\"), linewidth = 1.5) +\n  geom_line(aes(y = kurtosis_deviation, color = \"Kurtosis Deviation\"), linewidth = 1.5) +\n  geom_line(aes(y = distance_from_normal, color = \"Distance from Normal\"), linewidth = 1.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  geom_hline(yintercept = 1.0, linetype = \"dashed\", color = \"orange\", alpha = 0.7) +\n  labs(title = \"Convergence to Normality\",\n       subtitle = \"How quickly do distributions become 'practically normal'?\",\n       x = \"Number of Variables\", y = \"Deviation from Normal\",\n       color = \"Measure\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_color_manual(values = c(\"Skewness\" = \"blue\", \n                               \"Kurtosis Deviation\" = \"green\", \n                               \"Distance from Normal\" = \"purple\")) +\n  annotate(\"text\", x = 8, y = 0.6, label = \"|Skewness| &lt; 0.5\", color = \"red\", size = 3) +\n  annotate(\"text\", x = 8, y = 1.1, label = \"Distance &lt; 1.0\", color = \"orange\", size = 3)\n\nprint(convergence_plot)\n\n\n\n\n\n\n\n\n\nKey Insights from Simulation 2:\n\nSingle Variable: Starts with the original exponential distribution (skewed)\nTwo Variables: Sum becomes more symmetric but still skewed\nFive Variables: Much more bell-shaped, approaching normal\nTen Variables: Very close to normal distribution\nTheoretical Agreement: Observed means and standard deviations match theoretical predictions\nSkewness Reduction: As we add more variables, skewness approaches zero (normal distribution has zero skewness)"
  },
  {
    "objectID": "concepts/central-limit-theorem.html#applications",
    "href": "concepts/central-limit-theorem.html#applications",
    "title": "Central Limit Theorem",
    "section": "Applications",
    "text": "Applications\n\nShow how it allows you to make certain assumptions when working with a sample (because we typically don’t have access to a population)."
  },
  {
    "objectID": "concepts/central-limit-theorem.html#key-insights",
    "href": "concepts/central-limit-theorem.html#key-insights",
    "title": "Central Limit Theorem",
    "section": "Key Insights",
    "text": "Key Insights\n\nOriginal Distribution: We started with a highly skewed exponential distribution\nSampling Distribution: Even with a moderate sample size (n=30), the distribution of sample means is much more normal\nSample Size Effect: As sample size increases, the sampling distribution becomes:\n\nMore normal in shape\nNarrower (smaller standard error)\nMore concentrated around the true population mean\n\nCentral Limit Theorem: This demonstrates that regardless of the original distribution’s shape, the sampling distribution of the mean approaches normality"
  },
  {
    "objectID": "concepts/central-limit-theorem.html#mathematical-foundation",
    "href": "concepts/central-limit-theorem.html#mathematical-foundation",
    "title": "Central Limit Theorem",
    "section": "Mathematical Foundation",
    "text": "Mathematical Foundation\nThe Central Limit Theorem states that if \\(X_1, X_2, ..., X_n\\) are independent and identically distributed random variables with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then:\n\\(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} N(0,1)\\)\nWhere \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\) is the sample mean.\nThis means that for large enough sample sizes, the sampling distribution of the sample mean will be approximately normal with: - Mean = \\(\\mu\\) (population mean) - Standard Error = \\(\\frac{\\sigma}{\\sqrt{n}}\\) (population standard deviation divided by square root of sample size)"
  },
  {
    "objectID": "misc/papers/index.html",
    "href": "misc/papers/index.html",
    "title": "Analysis and Discussion of Papers",
    "section": "",
    "text": "This section provides analysis and discussion of various papers.\n\nChatbot Effects on Postpartum Mental Health"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is maintained by Nathan Ormond, a Senior Software Engineer at Highway Data Systems.\nNathan holds an MSc in Philosophy and is currently pursuing an MS in Statistics part-time. His diverse academic background and professional experience drive his passion for exploring and explaining complex statistical concepts.\nYou can find more about Nathan’s projects and work at https://linktr.ee/digitalgnosis.\n\n\nShow the code\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "about.html#about-the-author",
    "href": "about.html#about-the-author",
    "title": "About",
    "section": "",
    "text": "This site is maintained by Nathan Ormond, a Senior Software Engineer at Highway Data Systems.\nNathan holds an MSc in Philosophy and is currently pursuing an MS in Statistics part-time. His diverse academic background and professional experience drive his passion for exploring and explaining complex statistical concepts.\nYou can find more about Nathan’s projects and work at https://linktr.ee/digitalgnosis.\n\n\nShow the code\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Concepts",
    "section": "",
    "text": "This website provides interactive demonstrations and explanations of fundamental statistical concepts. Each page contains hands-on examples with R code that you can run and modify to deepen your understanding."
  },
  {
    "objectID": "index.html#welcome-to-statistics-concepts",
    "href": "index.html#welcome-to-statistics-concepts",
    "title": "Statistics Concepts",
    "section": "",
    "text": "This website provides interactive demonstrations and explanations of fundamental statistical concepts. Each page contains hands-on examples with R code that you can run and modify to deepen your understanding."
  },
  {
    "objectID": "index.html#available-topics",
    "href": "index.html#available-topics",
    "title": "Statistics Concepts",
    "section": "Available Topics",
    "text": "Available Topics\n\nCentral Limit Theorem\nUnderstanding how sampling distributions approach normality regardless of the original distribution’s shape.\nKey Learning Objectives: - Visualize the transformation from skewed to normal distributions - Explore the effect of sample size on sampling distributions - Understand the mathematical foundation of the CLT\n\n\nAnalysis and Discussion of Papers\nThis section provides analysis and discussion of various papers.\n\n\nComing Soon…\n\nConfidence Intervals: Understanding uncertainty in parameter estimation\nHypothesis Testing: The logic of statistical significance\nRegression Analysis: Modeling relationships between variables\nProbability Distributions: Common distributions and their properties"
  },
  {
    "objectID": "index.html#how-to-use-this-site",
    "href": "index.html#how-to-use-this-site",
    "title": "Statistics Concepts",
    "section": "How to Use This Site",
    "text": "How to Use This Site\n\nRead the Introduction: Each concept page starts with an overview\nRun the Code: Execute the R code blocks to see the demonstrations\nExperiment: Modify parameters to see how they affect the results\nUnderstand the Math: Review the mathematical foundations provided"
  },
  {
    "objectID": "index.html#technical-requirements",
    "href": "index.html#technical-requirements",
    "title": "Statistics Concepts",
    "section": "Technical Requirements",
    "text": "Technical Requirements\n\nR (version 4.0 or higher)\nRequired packages will be automatically installed when you run the code\nA modern web browser to view the rendered HTML"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Statistics Concepts",
    "section": "Getting Started",
    "text": "Getting Started\nClick on any topic above to begin exploring statistical concepts through interactive demonstrations!"
  },
  {
    "objectID": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html",
    "href": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html",
    "title": "Chatbot Effects on Postpartum Mental Health",
    "section": "",
    "text": "Show the code\n# Installation and Setup\nrequired_packages &lt;- c(\"ggplot2\", \"dplyr\", \"tidyr\", \"gridExtra\", \"moments\", \"nortest\")\n\n# Load required packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(moments)\nlibrary(nortest)\n\nset.seed(123)\n\n# Helper function to add color sample spots to captions\nadd_color_samples &lt;- function(caption, colors) {\n  # Create color sample spots using colored text with square brackets\n  color_spots &lt;- sapply(colors, function(color) {\n    paste0(\"[\", color, \"]\")  # Square brackets around color name\n  })\n  \n  # Replace color names with color spots\n  for(i in seq_along(colors)) {\n    color_name &lt;- names(colors)[i]\n    color_spot &lt;- color_spots[i]\n    caption &lt;- gsub(paste0(color_name, \" line\"), paste0(color_spot, \" line\"), caption, ignore.case = TRUE)\n    caption &lt;- gsub(paste0(color_name, \" =\"), paste0(color_spot, \" =\"), caption, ignore.case = TRUE)\n  }\n  \n  return(caption)\n}"
  },
  {
    "objectID": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#intro",
    "href": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#intro",
    "title": "Chatbot Effects on Postpartum Mental Health",
    "section": "Intro",
    "text": "Intro\n\nData from study by Suharwardy et al. (2023)"
  },
  {
    "objectID": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#design",
    "href": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#design",
    "title": "Chatbot Effects on Postpartum Mental Health",
    "section": "Design",
    "text": "Design\n\nAn unblinded randomized controlled trial was conducted at a tertiary academic center. English-speaking postpartum women aged 18 years or above with a live birth and access to a smartphone were eligible for enrollment prior to discharge from delivery hospitalization. Baseline surveys were administered to all participants prior to randomization to a mental health chatbot intervention or to usual care only. The intervention group downloaded the mental health chatbot smartphone application with perinatal-specific content, in addition to continuing usual care. Usual care consisted of routine postpartum follow up and mental health care as dictated by the patient’s obstetric provider. Surveys were administered during delivery hospitalization (baseline) and at 2-, 4-, and 6-weeks postpartum to assess depression and anxiety symptoms. The primary outcome was a change in depression symptoms at 6-weeks as measured using two depression screening tools: Patient Health Questionnaire-9 and Edinburgh Postnatal Depression Scale. Secondary outcomes included anxiety symptoms measured using Generalized Anxiety Disorder-7, and satisfaction and acceptability using validated scales. Based on a prior study, we estimated a sample size of 130 would have sufficient (80%) power to detect a moderate effect size (d=.4) in between group difference on the Patient Health Questionnaire-9."
  },
  {
    "objectID": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#results",
    "href": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#results",
    "title": "Chatbot Effects on Postpartum Mental Health",
    "section": "Results",
    "text": "Results\n\nA total of 192 women were randomized equally 1:1 to the chatbot or usual care; of these, 152 women completed the 6-week survey (n=68 chatbot, n=84 usual care) and were included in the final analysis. Mean baseline mental health assessment scores were below positive screening thresholds. At 6-weeks, there was a greater decrease in Patient Health Questionnaire-9 scores among the chatbot group compared to the usual care group (mean decrease=1.32, standard deviation=3.4 vs mean decrease=0.13, standard deviation=3.01, respectively). 6-week mean Edinburgh Postnatal Depression Scale and Generalized Anxiety Disorder-7 scores did not differ between groups and were similar to baseline. 91% (n=62) of the chatbot users were satisfied or highly satisfied with the chatbot, and 74% (n=50) of the intervention group reported use of the chatbot at least once in 2 weeks prior to the 6-week survey. 80% of study participants reported being comfortable with the use of a mobile smartphone application for mood management."
  },
  {
    "objectID": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#conclusion",
    "href": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#conclusion",
    "title": "Chatbot Effects on Postpartum Mental Health",
    "section": "Conclusion",
    "text": "Conclusion\n\nUse of a chatbot was acceptable to women in the early postpartum period. The sample did not screen positive for depression at baseline and thus the potential of the chatbot to reduce depressive symptoms in this population was limited. This study was conducted in a general obstetric population. Future studies of longer duration in high-risk postpartum populations who screen positive for depression are needed to further understand the utility and efficacy of such digital therapeutics for that population.\n\n| .97 |"
  },
  {
    "objectID": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#visualizations",
    "href": "misc/papers/chatbot-mental-health/chatbot-postpartum-mental-health.html#visualizations",
    "title": "Chatbot Effects on Postpartum Mental Health",
    "section": "Visualizations",
    "text": "Visualizations\n\n\nShow the code\n# Create simulated data based on the paper's results\nset.seed(456)\n\n# Sample sizes\nn_chatbot &lt;- 68\nn_usual_care &lt;- 84\n\n# Simulate baseline and 6-week PHQ-9 scores based on paper results\n# Baseline: below positive screening thresholds (PHQ-9 &lt; 10)\n# 6-week: chatbot decrease = 1.32, usual care decrease = 0.13\n\n# Baseline scores (below screening threshold)\nbaseline_chatbot &lt;- rnorm(n_chatbot, mean = 6.5, sd = 2.5)\nbaseline_usual &lt;- rnorm(n_usual_care, mean = 6.8, sd = 2.3)\n\n# 6-week scores (with treatment effect)\nweek6_chatbot &lt;- baseline_chatbot - rnorm(n_chatbot, mean = 1.32, sd = 3.4)\nweek6_usual &lt;- baseline_usual - rnorm(n_usual_care, mean = 0.13, sd = 3.01)\n\n# Simulate EPDS scores (non-significant results)\n# Paper states: \"6-week mean Edinburgh Postnatal Depression Scale and Generalized Anxiety Disorder-7 scores did not differ between groups\"\n# EPDS range: 0-30, similar baseline to PHQ-9 but smaller/no treatment effect\nbaseline_epds_chatbot &lt;- rnorm(n_chatbot, mean = 7.2, sd = 2.8)\nbaseline_epds_usual &lt;- rnorm(n_usual_care, mean = 7.5, sd = 2.6)\n\n# 6-week EPDS scores (minimal/no treatment effect)\nweek6_epds_chatbot &lt;- baseline_epds_chatbot - rnorm(n_chatbot, mean = 0.3, sd = 2.8)  # Small effect\nweek6_epds_usual &lt;- baseline_epds_usual - rnorm(n_usual_care, mean = 0.2, sd = 2.7)   # Similar small effect\n\n# Create data frame\ntherapy_data &lt;- data.frame(\n  group = rep(c(\"Chatbot\", \"Usual Care\"), c(n_chatbot, n_usual_care)),\n  baseline_phq9 = c(baseline_chatbot, baseline_usual),\n  week6_phq9 = c(week6_chatbot, week6_usual),\n  change_phq9 = c(week6_chatbot - baseline_chatbot, week6_usual - baseline_usual),\n  baseline_epds = c(baseline_epds_chatbot, baseline_epds_usual),\n  week6_epds = c(week6_epds_chatbot, week6_epds_usual),\n  change_epds = c(week6_epds_chatbot - baseline_epds_chatbot, week6_epds_usual - baseline_epds_usual)\n)\n\n# Add participant ID\ntherapy_data$participant_id &lt;- 1:nrow(therapy_data)\n\n\n\n\nShow the code\n# Visualization 1: Change in PHQ-9 scores by group\nchange_plot &lt;- ggplot(therapy_data, aes(x = group, y = change_phq9, fill = group)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.6, size = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  # Add colored rectangle for red line legend in top-right corner\n  annotate(\"rect\", xmin = 2.3, xmax = 2.5, ymin = 6, ymax = 6.5, \n           fill = \"red\", alpha = 0.7) +\n  annotate(\"text\", x = 2.6, y = 6.25, label = \"= No change\", size = 3) +\n  labs(title = \"Change in Depression Symptoms (PHQ-9)\",\n       subtitle = \"Negative values indicate improvement in symptoms\",\n       x = \"Treatment Group\", y = \"Change in PHQ-9 Score\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"none\") +\n  scale_fill_manual(values = c(\"Chatbot\" = \"steelblue\", \"Usual Care\" = \"orange\"))\n\nprint(change_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 2: Baseline vs 6-week scores\n# Reshape data for plotting\ntherapy_long &lt;- therapy_data %&gt;%\n  select(participant_id, group, baseline_phq9, week6_phq9) %&gt;%\n  pivot_longer(cols = c(baseline_phq9, week6_phq9), \n               names_to = \"timepoint\", \n               values_to = \"phq9_score\") %&gt;%\n  mutate(timepoint = factor(timepoint, \n                           levels = c(\"baseline_phq9\", \"week6_phq9\"),\n                           labels = c(\"Baseline\", \"6 Weeks\")))\n\n# Create two separate charts for better clarity\n\n# Chart 1: Group means only (clean and clear)\n# Calculate means manually to avoid legend issues\nmeans_data &lt;- therapy_long %&gt;%\n  group_by(timepoint, group) %&gt;%\n  summarise(\n    mean_score = mean(phq9_score, na.rm = TRUE),\n    se = sd(phq9_score, na.rm = TRUE) / sqrt(n()),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(\n    ci_lower = mean_score - 1.96 * se,\n    ci_upper = mean_score + 1.96 * se\n  )\n\nmeans_plot &lt;- ggplot(means_data, aes(x = timepoint, y = mean_score, color = group, group = group)) +\n  # Add confidence intervals for group means\n  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper, fill = group), alpha = 0.3, color = NA) +\n  # Group means with prominence\n  geom_line(linewidth = 3, alpha = 1) +\n  geom_point(size = 5, alpha = 1) +\n  # Add mean values and sample sizes with horizontal offset\n  geom_text(aes(label = round(mean_score, 1), x = as.numeric(timepoint) + ifelse(group == \"Chatbot\", -0.15, 0.15)), \n            vjust = -1.8, size = 4, fontface = \"bold\") +\n  geom_text(aes(label = \"(n=68)\", x = as.numeric(timepoint) + ifelse(group == \"Chatbot\", -0.15, 0.15)), \n            vjust = -0.8, size = 3) +\n  labs(title = \"Group Means\",\n       subtitle = \"Mean PHQ-9 scores with 95% confidence intervals\",\n       x = \"Time Point\", y = \"PHQ-9 Score\",\n       caption = \"Lower scores = Better outcomes\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 12),\n        plot.subtitle = element_text(hjust = 0.5, size = 9),\n        legend.position = \"top\") +\n  scale_color_manual(values = c(\"Chatbot\" = \"steelblue\", \"Usual Care\" = \"orange\")) +\n  scale_fill_manual(values = c(\"Chatbot\" = \"steelblue\", \"Usual Care\" = \"orange\"))\n\n# Chart 2: Individual trajectories only\ntrajectories_plot &lt;- ggplot(therapy_long, aes(x = timepoint, y = phq9_score, color = group)) +\n  # Individual trajectories with better visibility\n  geom_line(aes(group = participant_id), alpha = 0.4, linewidth = 0.5) +\n  geom_point(aes(group = participant_id), alpha = 0.6, size = 1.2) +\n  # Add group means as reference (thinner)\n  stat_summary(aes(group = group), fun = mean, geom = \"line\", \n               linewidth = 2, alpha = 1, linetype = \"dashed\") +\n  stat_summary(aes(group = group), fun = mean, geom = \"point\", \n               size = 3, alpha = 1, shape = 21, fill = \"white\") +\n  labs(title = \"Individual Trajectories\",\n       subtitle = \"Individual participant trajectories with group means (dashed lines)\",\n       x = \"Time Point\", y = \"PHQ-9 Score\",\n       caption = \"Each line = One participant | Dashed lines = Group means\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 12),\n        plot.subtitle = element_text(hjust = 0.5, size = 9),\n        legend.position = \"top\") +\n  scale_color_manual(values = c(\"Chatbot\" = \"steelblue\", \"Usual Care\" = \"orange\"))\n\n# Display both charts side by side using grid.arrange\nlibrary(gridExtra)\ngrid.arrange(means_plot, trajectories_plot, ncol = 2, \n             top = \"Depression Symptoms Over Time: Group Means vs Individual Trajectories\",\n             heights = c(0.9, 0.1))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 3: Distribution of baseline scores\nbaseline_dist_plot &lt;- ggplot(therapy_data, aes(x = baseline_phq9, fill = group)) +\n  geom_histogram(bins = 15, alpha = 0.7, position = \"identity\") +\n  geom_vline(xintercept = 10, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  # Add colored rectangle for red line legend in top-right corner\n  annotate(\"rect\", xmin = 11.5, xmax = 12, ymin = 9, ymax = 9.5, \n           fill = \"red\", alpha = 0.7) +\n  annotate(\"text\", x = 12.1, y = 9.25, label = \"= Screening threshold (≥10)\", size = 3) +\n  labs(title = \"Distribution of Baseline Depression Scores\",\n       subtitle = \"PHQ-9 screening threshold (≥10 indicates depression)\",\n       x = \"Baseline PHQ-9 Score\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_fill_manual(values = c(\"Chatbot\" = \"steelblue\", \"Usual Care\" = \"orange\"))\n\nprint(baseline_dist_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 4: Demographics comparison with contextual information\n# Create demographic data based on the table\ndemographics_data &lt;- data.frame(\n  variable = rep(c(\"Age 25-34\", \"Age 35-44\", \"Married\", \"Employed\", \n                   \"Mental Health History\", \"Comfortable with Apps\"), each = 2),\n  group = rep(c(\"Chatbot\", \"Usual Care\"), 6),\n  percentage = c(63.2, 47.6, 32.4, 44.0, 94.1, 83.3, 80.9, 78.6, 16.2, 9.5, 82.4, 77.4)\n)\n\n# Add statistical significance indicators - mark age 25-34 and married as significant\ndemographics_data$significant &lt;- c(TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE)\n\ndemographics_plot &lt;- ggplot(demographics_data, aes(x = variable, y = percentage, fill = group)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.8) +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), \n            position = position_dodge(width = 0.9), vjust = -0.5, size = 3) +\n  # Add explanatory annotations in better positions\n  annotate(\"text\", x = 0.8, y = 108, label = \"SIGNIFICANT DIFFERENCES\", \n           size = 3.5, fontface = \"bold\", color = \"red\", hjust = 0) +\n  annotate(\"text\", x = 0.8, y = 104, label = \"Age 25-34: p=0.02 | Married: p=0.046\", \n           size = 2.8, color = \"red\", hjust = 0) +\n  annotate(\"text\", x = 0.8, y = 100, label = \"Groups not perfectly balanced\", \n           size = 2.5, color = \"darkred\", hjust = 0) +\n  annotate(\"text\", x = 0.8, y = 96, label = \"May affect treatment interpretation\", \n           size = 2.5, color = \"darkred\", hjust = 0) +\n  labs(title = \"Demographic Characteristics by Group\",\n       subtitle = \"Percentage of participants in each category | * = Statistically significant difference (p &lt; 0.05)\",\n       x = \"Characteristic\", y = \"Percentage (%)\",\n       caption = \"IMPORTANT: Demographic differences may confound treatment effects\\nChatbot group is younger and more likely to be married\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"Chatbot\" = \"steelblue\", \"Usual Care\" = \"orange\")) +\n  # Extend y-axis to accommodate labels and annotations\n  scale_y_continuous(limits = c(0, 115), expand = c(0, 0))\n\nprint(demographics_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 5: Treatment satisfaction and usage\nsatisfaction_data &lt;- data.frame(\n  metric = c(\"Satisfied with Chatbot\", \"Used Chatbot in Past 2 Weeks\", \n             \"Comfortable with Mental Health Apps\"),\n  percentage = c(91, 74, 80),\n  group = c(\"Chatbot Users\", \"Chatbot Users\", \"All Participants\")\n)\n\nsatisfaction_plot &lt;- ggplot(satisfaction_data, aes(x = metric, y = percentage, fill = group)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  geom_text(aes(label = paste0(percentage, \"%\")), vjust = -0.5, size = 4) +\n  labs(title = \"Treatment Acceptability and Usage\",\n       subtitle = \"Percentage of participants reporting positive experiences\",\n       x = \"Metric\", y = \"Percentage (%)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"Chatbot Users\" = \"steelblue\", \"All Participants\" = \"lightblue\")) +\n  # Extend y-axis to accommodate labels\n  scale_y_continuous(limits = c(0, 100), expand = c(0, 0))\n\nprint(satisfaction_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 6: Effect size and confidence intervals\n# Calculate effect size (Cohen's d) for the change in PHQ-9\nchatbot_change &lt;- therapy_data$change_phq9[therapy_data$group == \"Chatbot\"]\nusual_change &lt;- therapy_data$change_phq9[therapy_data$group == \"Usual Care\"]\n\n# Pooled standard deviation\nn1 &lt;- length(chatbot_change)\nn2 &lt;- length(usual_change)\nvar1 &lt;- var(chatbot_change)\nvar2 &lt;- var(usual_change)\npooled_sd &lt;- sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1 + n2 - 2))\n\n# Cohen's d\ncohens_d &lt;- (mean(chatbot_change) - mean(usual_change)) / pooled_sd\n\n# Create effect size plot\neffect_data &lt;- data.frame(\n  group = c(\"Chatbot\", \"Usual Care\"),\n  mean_change = c(mean(chatbot_change), mean(usual_change)),\n  se = c(sd(chatbot_change)/sqrt(n1), sd(usual_change)/sqrt(n2))\n)\n\neffect_plot &lt;- ggplot(effect_data, aes(x = group, y = mean_change, fill = group)) +\n  geom_bar(stat = \"identity\", alpha = 0.7) +\n  geom_errorbar(aes(ymin = mean_change - 1.96*se, ymax = mean_change + 1.96*se), \n                width = 0.2, linewidth = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Treatment Effect on Depression Symptoms\",\n       subtitle = paste(\"Cohen's d =\", round(cohens_d, 3), \n                       \"| Error bars = 95% confidence intervals\"),\n       x = \"Treatment Group\", y = \"Mean Change in PHQ-9 Score\",\n       caption = \"Negative values = Improvement in symptoms\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"none\") +\n  scale_fill_manual(values = c(\"Chatbot\" = \"steelblue\", \"Usual Care\" = \"orange\"))\n\nprint(effect_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 7: Statistical significance and effect size summary\n# Calculate t-test for significance\nt_test_result &lt;- t.test(change_phq9 ~ group, data = therapy_data)\np_value &lt;- t_test_result$p.value\n\n# Create significance and effect size summary\nsignificance_data &lt;- data.frame(\n  metric = c(\"Mean Change (Chatbot)\", \"Mean Change (Usual Care)\", \n             \"Difference\", \"Cohen's d\", \"P-value\"),\n  value = c(mean(chatbot_change), mean(usual_change), \n            mean(chatbot_change) - mean(usual_change), cohens_d, p_value),\n  interpretation = c(\"Improvement\", \"Minimal change\", \n                     \"Treatment effect\", \"Effect size\", \"Significance\")\n)\n\n# Create significance plot\nsignificance_plot &lt;- ggplot(significance_data[1:3, ], aes(x = metric, y = value, fill = metric)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  geom_text(aes(label = paste0(round(value, 2), \n                               ifelse(metric == \"P-value\", \n                                      paste0(\" (p = \", round(p_value, 4), \")\"), \n                                      \"\"))), \n            vjust = -0.5, size = 4) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Treatment Effect: Statistical Significance\",\n       subtitle = paste(\"P-value =\", round(p_value, 4), \n                       \"| Significant:\", ifelse(p_value &lt; 0.05, \"YES\", \"NO\")),\n       x = \"Metric\", y = \"Change in PHQ-9 Score\",\n       caption = \"Negative values = Improvement in depression symptoms\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\") +\n  scale_fill_manual(values = c(\"Mean Change (Chatbot)\" = \"steelblue\", \n                              \"Mean Change (Usual Care)\" = \"orange\",\n                              \"Difference\" = \"purple\"))\n\nprint(significance_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 8: Effect size interpretation with confidence intervals\n# Calculate confidence intervals for the difference\ndiff_ci &lt;- t.test(chatbot_change, usual_change)$conf.int\n\neffect_size_data &lt;- data.frame(\n  measure = c(\"Cohen's d\", \"Difference (95% CI)\"),\n  value = c(cohens_d, mean(chatbot_change) - mean(usual_change)),\n  lower = c(cohens_d - 0.1, diff_ci[1]),  # Approximate CI for Cohen's d\n  upper = c(cohens_d + 0.1, diff_ci[2]),\n  interpretation = c(\"Standardized effect size\", \"Raw difference in PHQ-9 scores\")\n)\n\neffect_size_plot &lt;- ggplot(effect_size_data, aes(x = measure, y = value, fill = measure)) +\n  geom_bar(stat = \"identity\", alpha = 0.7) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, linewidth = 1) +\n  geom_text(aes(label = paste0(round(value, 3), \n                               ifelse(measure == \"Difference (95% CI)\", \n                                      paste0(\"\\n[\", round(lower, 3), \", \", round(upper, 3), \"]\"), \n                                      \"\"))), \n            vjust = -0.5, size = 3.5) +\n  labs(title = \"Effect Size and Confidence Intervals\",\n       subtitle = paste(\"Cohen's d interpretation:\", \n                       ifelse(abs(cohens_d) &lt; 0.2, \"Negligible\", \n                              ifelse(abs(cohens_d) &lt; 0.5, \"Small\", \n                                     ifelse(abs(cohens_d) &lt; 0.8, \"Medium\", \"Large\"))), \"effect\"),\n       x = \"Measure\", y = \"Value\",\n       caption = \"Error bars = 95% confidence intervals\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"none\") +\n  scale_fill_manual(values = c(\"Cohen's d\" = \"darkgreen\", \"Difference (95% CI)\" = \"purple\"))\n\nprint(effect_size_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 9: Clinical significance - proportion improved\n# Define clinical improvement (PHQ-9 decrease of 5+ points or 50% reduction)\ntherapy_data$clinically_improved &lt;- ifelse(therapy_data$change_phq9 &lt;= -5 | \n                                          (therapy_data$baseline_phq9 &gt;= 10 & \n                                           therapy_data$change_phq9 &lt;= -0.5 * therapy_data$baseline_phq9), \n                                          \"Improved\", \"Not Improved\")\n\nclinical_summary &lt;- therapy_data %&gt;%\n  group_by(group, clinically_improved) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  group_by(group) %&gt;%\n  mutate(percentage = count / sum(count) * 100)\n\nclinical_plot &lt;- ggplot(clinical_summary, aes(x = group, y = percentage, fill = clinically_improved)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\\n(n=\", count, \")\")), \n            position = position_stack(vjust = 0.5), size = 3.5) +\n  labs(title = \"Clinical Significance: Proportion with Meaningful Improvement\",\n       subtitle = \"Defined as ≥5 point decrease or ≥50% reduction from baseline\",\n       x = \"Treatment Group\", y = \"Percentage (%)\",\n       fill = \"Clinical Status\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_fill_manual(values = c(\"Improved\" = \"darkgreen\", \"Not Improved\" = \"lightgray\"))\n\nprint(clinical_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 10: Forest plot of key results\n# Create forest plot data\nforest_data &lt;- data.frame(\n  outcome = c(\"PHQ-9 Change\", \"Baseline PHQ-9\", \"6-week PHQ-9\", \"Age 25-34\", \"Married\"),\n  chatbot_mean = c(mean(chatbot_change), mean(baseline_chatbot), mean(week6_chatbot), 63.2, 94.1),\n  usual_care_mean = c(mean(usual_change), mean(baseline_usual), mean(week6_usual), 47.6, 83.3),\n  difference = c(mean(chatbot_change) - mean(usual_change), \n                mean(baseline_chatbot) - mean(baseline_usual),\n                mean(week6_chatbot) - mean(week6_usual), 15.6, 10.8),\n  p_value = c(p_value, 0.5, 0.3, 0.02, 0.046)  # Approximate p-values\n)\n\nforest_plot &lt;- ggplot(forest_data, aes(x = outcome, y = difference, color = p_value &lt; 0.05)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = difference - 0.5, ymax = difference + 0.5), width = 0.2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  geom_text(aes(label = paste0(\"p = \", round(p_value, 3))), \n            vjust = -0.8, size = 3) +\n  # Add colored rectangles for legend in top-right area\n  annotate(\"rect\", xmin = 4.8, xmax = 5, ymin = 12, ymax = 12.5, \n           fill = \"red\", alpha = 0.7) +\n  annotate(\"text\", x = 5.1, y = 12.25, label = \"= No difference\", size = 2.5) +\n  annotate(\"rect\", xmin = 4.8, xmax = 5, ymin = 11, ymax = 11.5, \n           fill = \"darkgreen\", alpha = 0.7) +\n  annotate(\"text\", x = 5.1, y = 11.25, label = \"= Significant\", size = 2.5) +\n  annotate(\"rect\", xmin = 4.8, xmax = 5, ymin = 10, ymax = 10.5, \n           fill = \"orange\", alpha = 0.7) +\n  annotate(\"text\", x = 5.1, y = 10.25, label = \"= Not significant\", size = 2.5) +\n  coord_flip() +\n  labs(title = \"Forest Plot: Key Differences Between Groups\",\n       subtitle = \"Statistical significance indicated by color\",\n       x = \"Outcome\", y = \"Difference (Chatbot - Usual Care)\",\n       caption = \"Positive values favor chatbot group\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"none\") +\n  scale_color_manual(values = c(\"TRUE\" = \"darkgreen\", \"FALSE\" = \"orange\"))\n\nprint(forest_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 11: Null hypothesis testing visualization\n# Simulate null distribution (no difference between groups)\nset.seed(789)\nn_simulations &lt;- 10000\nnull_differences &lt;- numeric(n_simulations)\n\n# Permutation test: randomly shuffle group labels\nfor(i in 1:n_simulations) {\n  shuffled_data &lt;- therapy_data\n  shuffled_data$group &lt;- sample(shuffled_data$group)\n  \n  # Calculate difference with shuffled groups\n  chatbot_shuffled &lt;- shuffled_data$change_phq9[shuffled_data$group == \"Chatbot\"]\n  usual_shuffled &lt;- shuffled_data$change_phq9[shuffled_data$group == \"Usual Care\"]\n  \n  null_differences[i] &lt;- mean(chatbot_shuffled) - mean(usual_shuffled)\n}\n\n# Calculate observed difference\nobserved_diff &lt;- mean(chatbot_change) - mean(usual_change)\n\n# Create null distribution plot\nnull_plot &lt;- ggplot(data.frame(null_diff = null_differences), aes(x = null_diff)) +\n  geom_histogram(bins = 50, fill = \"lightgray\", alpha = 0.7, color = \"black\") +\n  geom_vline(xintercept = observed_diff, color = \"red\", linewidth = 2) +\n  geom_vline(xintercept = -observed_diff, color = \"red\", linewidth = 2, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, color = \"blue\", linewidth = 1, linetype = \"dotted\") +\n  # Add colored rectangles for legend in top-right corner\n  annotate(\"rect\", xmin = 1.8, xmax = 2, ymin = 800, ymax = 850, \n           fill = \"red\", alpha = 0.7) +\n  annotate(\"text\", x = 2.1, y = 825, label = \"= Observed difference\", size = 3) +\n  annotate(\"rect\", xmin = 1.8, xmax = 2, ymin = 750, ymax = 800, \n           fill = \"blue\", alpha = 0.7) +\n  annotate(\"text\", x = 2.1, y = 775, label = \"= H0 (no difference)\", size = 3) +\n  labs(title = \"Null Hypothesis Testing: Distribution Under H0\",\n       subtitle = paste(\"Observed difference =\", round(observed_diff, 3), \n                       \"| P-value =\", round(mean(abs(null_differences) &gt;= abs(observed_diff)), 4)),\n       x = \"Difference in Mean Change (Chatbot - Usual Care)\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\nprint(null_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 12: P-value interpretation with effect size\n# Calculate exact p-value from t-test\nt_test_exact &lt;- t.test(chatbot_change, usual_change)\nexact_p_value &lt;- t_test_exact$p.value\n\n# Create p-value interpretation plot\np_value_data &lt;- data.frame(\n  threshold = c(\"p &lt; 0.001\", \"p &lt; 0.01\", \"p &lt; 0.05\", \"p &lt; 0.10\"),\n  value = c(0.001, 0.01, 0.05, 0.10),\n  interpretation = c(\"Highly Significant\", \"Very Significant\", \"Significant\", \"Marginally Significant\")\n)\n\np_value_plot &lt;- ggplot(p_value_data, aes(x = threshold, y = value, fill = value &lt; exact_p_value)) +\n  geom_bar(stat = \"identity\", alpha = 0.7) +\n  geom_hline(yintercept = exact_p_value, color = \"red\", linewidth = 2) +\n  geom_text(aes(label = paste0(\"p = \", round(value, 3))), vjust = -0.5, size = 3) +\n  geom_text(aes(x = 2.5, y = exact_p_value + 0.01, \n                label = paste0(\"Observed p = \", round(exact_p_value, 4))), \n            color = \"red\", size = 4) +\n  labs(title = \"P-Value Interpretation: Where Does Our Result Fall?\",\n       subtitle = paste(\"Observed p-value =\", round(exact_p_value, 4), \n                       \"| Significance level = 0.05\"),\n       x = \"Significance Threshold\", y = \"P-Value\",\n       caption = add_color_samples(\"Red line = Observed p-value | Bars = Significance thresholds\", c(\"Red\" = \"red\"))) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"none\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"darkgreen\", \"FALSE\" = \"orange\"))\n\nprint(p_value_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 13: Confidence intervals and null hypothesis\n# Calculate confidence intervals\nci_95 &lt;- t.test(chatbot_change, usual_change)$conf.int\nci_90 &lt;- t.test(chatbot_change, usual_change, conf.level = 0.90)$conf.int\nci_99 &lt;- t.test(chatbot_change, usual_change, conf.level = 0.99)$conf.int\n\nci_data &lt;- data.frame(\n  level = c(\"99% CI\", \"95% CI\", \"90% CI\"),\n  lower = c(ci_99[1], ci_95[1], ci_90[1]),\n  upper = c(ci_99[2], ci_95[2], ci_90[2]),\n  width = c(ci_99[2] - ci_99[1], ci_95[2] - ci_95[1], ci_90[2] - ci_90[1])\n)\n\nci_plot &lt;- ggplot(ci_data, aes(y = level)) +\n  geom_errorbar(aes(xmin = lower, xmax = upper, color = level), \n                linewidth = 2, width = 0.3) +\n  geom_vline(xintercept = 0, color = \"red\", linewidth = 2, linetype = \"dashed\") +\n  geom_vline(xintercept = observed_diff, color = \"blue\", linewidth = 2) +\n  geom_text(aes(x = lower - 0.1, label = paste0(\"[\", round(lower, 3), \", \", round(upper, 3), \"]\")), \n            hjust = 1, size = 3) +\n  labs(title = \"Confidence Intervals and Null Hypothesis\",\n       subtitle = paste(\"Observed difference =\", round(observed_diff, 3), \n                       \"| Does 0 fall within the CI?\"),\n       x = \"Difference in Mean Change (Chatbot - Usual Care)\", y = \"Confidence Level\",\n       caption = add_color_samples(\"Red line = H0 (no difference) | Blue line = Observed difference | CI excludes 0 = Significant\", \n                                 c(\"Red\" = \"red\", \"Blue\" = \"blue\"))) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_color_manual(values = c(\"99% CI\" = \"darkred\", \"95% CI\" = \"red\", \"90% CI\" = \"orange\"))\n\nprint(ci_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 14: Power analysis and effect size relationship\n# Calculate power for different effect sizes\neffect_sizes &lt;- seq(0.1, 1.0, by = 0.1)\npower_values &lt;- numeric(length(effect_sizes))\n\nfor(i in 1:length(effect_sizes)) {\n  # Approximate power calculation\n  n_per_group &lt;- min(length(chatbot_change), length(usual_change))\n  power_values[i] &lt;- power.t.test(n = n_per_group, d = effect_sizes[i], \n                                 sig.level = 0.05, type = \"two.sample\")$power\n}\n\npower_data &lt;- data.frame(effect_size = effect_sizes, power = power_values)\n\npower_plot &lt;- ggplot(power_data, aes(x = effect_size, y = power)) +\n  geom_line(linewidth = 2, color = \"blue\") +\n  geom_point(size = 3, color = \"blue\") +\n  geom_vline(xintercept = abs(cohens_d), color = \"red\", linewidth = 2, linetype = \"dashed\") +\n  geom_hline(yintercept = 0.8, color = \"green\", linewidth = 1, linetype = \"dashed\") +\n  # Add explanation for the 80% threshold\n  annotate(\"text\", x = 0.05, y = 0.82, \n           label = \"80% POWER THRESHOLD\", size = 3, fontface = \"bold\", color = \"darkgreen\", hjust = 0) +\n  annotate(\"text\", x = 0.05, y = 0.78, \n           label = \"Industry standard for reliable\", size = 2.5, color = \"darkgreen\", hjust = 0) +\n  annotate(\"text\", x = 0.05, y = 0.75, \n           label = \"detection of effects\", size = 2.5, color = \"darkgreen\", hjust = 0) +\n  annotate(\"text\", x = 0.05, y = 0.72, \n           label = \"Below 80% = Underpowered study\", size = 2.5, color = \"darkgreen\", hjust = 0) +\n  annotate(\"text\", x = 0.05, y = 0.69, \n           label = \"Above 80% = Adequate power\", size = 2.5, color = \"darkgreen\", hjust = 0) +\n  geom_text(aes(x = abs(cohens_d) + 0.05, y = 0.5, \n                label = paste0(\"Observed\\neffect size\\n= \", round(abs(cohens_d), 3))), \n            color = \"red\", size = 3) +\n  # Add comprehensive annotations explaining power analysis (moved to right side)\n  annotate(\"text\", x = 0.65, y = 0.9, \n           label = \"WHAT THIS SHOWS:\", size = 3.5, fontface = \"bold\", hjust = 0) +\n  annotate(\"text\", x = 0.65, y = 0.85, \n           label = paste0(\"• Current study power = \", \n                         round(power.t.test(n = min(length(chatbot_change), length(usual_change)), \n                                           d = abs(cohens_d), sig.level = 0.05, \n                                           type = \"two.sample\")$power * 100, 1), \"%\"), \n           size = 3, hjust = 0) +\n  annotate(\"text\", x = 0.65, y = 0.8, \n           label = paste0(\"• Observed effect size = \", round(abs(cohens_d), 3)), \n           size = 3, hjust = 0) +\n  annotate(\"text\", x = 0.65, y = 0.75, \n           label = paste0(\"• Effect interpretation = \", \n                         ifelse(abs(cohens_d) &lt; 0.2, \"Negligible\", \n                                ifelse(abs(cohens_d) &lt; 0.5, \"Small\", \n                                       ifelse(abs(cohens_d) &lt; 0.8, \"Medium\", \"Large\")))), \n           size = 3, hjust = 0) +\n  # Add future study recommendations\n  annotate(\"text\", x = 0.65, y = 0.6, \n           label = \"FUTURE STUDIES NEED:\", size = 3.5, fontface = \"bold\", hjust = 0, color = \"darkblue\") +\n  annotate(\"text\", x = 0.65, y = 0.55, \n           label = \"• Larger sample size for higher power\", \n           size = 3, hjust = 0, color = \"darkblue\") +\n  annotate(\"text\", x = 0.65, y = 0.5, \n           label = \"• Target population with higher depression scores\", \n           size = 3, hjust = 0, color = \"darkblue\") +\n  annotate(\"text\", x = 0.65, y = 0.45, \n           label = \"• Longer follow-up period\", \n           size = 3, hjust = 0, color = \"darkblue\") +\n  annotate(\"text\", x = 0.65, y = 0.4, \n           label = \"• Stratified randomisation for balanced groups\", \n           size = 3, hjust = 0, color = \"darkblue\") +\n  # Add sample size calculation for 80% power\n  annotate(\"text\", x = 0.65, y = 0.25, \n           label = \"SAMPLE SIZE FOR 80% POWER:\", size = 3.5, fontface = \"bold\", hjust = 0, color = \"darkgreen\") +\n  annotate(\"text\", x = 0.65, y = 0.2, \n           label = paste0(\"• For d = 0.3 (small effect): ~350 per group\"), \n           size = 3, hjust = 0, color = \"darkgreen\") +\n  annotate(\"text\", x = 0.65, y = 0.15, \n           label = paste0(\"• For d = 0.5 (medium effect): ~130 per group\"), \n           size = 3, hjust = 0, color = \"darkgreen\") +\n  annotate(\"text\", x = 0.65, y = 0.1, \n           label = paste0(\"• For d = 0.8 (large effect): ~50 per group\"), \n           size = 3, hjust = 0, color = \"darkgreen\") +\n  labs(title = \"Power Analysis: Relationship Between Effect Size and Statistical Power\",\n       subtitle = paste(\"Current study power for observed effect size =\", \n                       round(power.t.test(n = min(length(chatbot_change), length(usual_change)), \n                                         d = abs(cohens_d), sig.level = 0.05, \n                                         type = \"two.sample\")$power, 3)),\n       x = \"Effect Size (Cohen's d)\", y = \"Statistical Power\",\n       caption = \"Red line = Observed effect size | Green line = 80% power threshold | Blue line = Power curve\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\nprint(power_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 15: Bayesian interpretation (posterior probability)\n# Simple Bayesian analysis using normal approximation\n# Prior: Normal(0, 1) - skeptical prior\n# Likelihood: Normal(observed_diff, SE)\n# Posterior: Normal with updated mean and variance\n\nse_diff &lt;- sqrt(var(chatbot_change)/length(chatbot_change) + var(usual_change)/length(usual_change))\nprior_mean &lt;- 0\nprior_sd &lt;- 1\nlikelihood_mean &lt;- observed_diff\nlikelihood_sd &lt;- se_diff\n\n# Posterior parameters\nposterior_precision &lt;- 1/prior_sd^2 + 1/likelihood_sd^2\nposterior_sd &lt;- sqrt(1/posterior_precision)\nposterior_mean &lt;- (prior_mean/prior_sd^2 + likelihood_mean/likelihood_sd^2) / posterior_precision\n\n# Calculate probability that effect &gt; 0\nprob_positive &lt;- 1 - pnorm(0, posterior_mean, posterior_sd)\n\n# Create Bayesian plot\nx_vals &lt;- seq(-2, 2, by = 0.01)\nprior_density &lt;- dnorm(x_vals, prior_mean, prior_sd)\nlikelihood_density &lt;- dnorm(x_vals, likelihood_mean, likelihood_sd)\nposterior_density &lt;- dnorm(x_vals, posterior_mean, posterior_sd)\n\nbayesian_data &lt;- data.frame(\n  x = rep(x_vals, 3),\n  density = c(prior_density, likelihood_density, posterior_density),\n  distribution = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(x_vals))\n)\n\n# Define annotation positions for easy adjustment\nposterior_x &lt;- -1.99\nposterior_y_start &lt;- 0.8\n\nlikelihood_x &lt;- -1.99\nlikelihood_y_start &lt;- 0.5\n\ninterpretation_x &lt;- -0.4\ninterpretation_y_start &lt;- 0.85\n\nnull_x &lt;- 0.1\nnull_y_start &lt;- 0.6\n\nprior_x &lt;- 0.8\nprior_y_start &lt;- 0.4\n\nbayesian_plot &lt;- ggplot(bayesian_data, aes(x = x, y = density, color = distribution)) +\n  geom_line(linewidth = 1.5) +\n  geom_vline(xintercept = 0, color = \"red\", linewidth = 1, linetype = \"dashed\") +\n  geom_vline(xintercept = posterior_mean, color = \"blue\", linewidth = 2) +\n  # Reposition posterior mean text to avoid overlap and fix blurriness\n  annotate(\"text\", x = posterior_mean + 0.1, y = max(posterior_density) + 0.05, \n           label = paste0(\"Posterior mean = \", round(posterior_mean, 3), \" (confidence = \", round(dnorm(posterior_mean, posterior_mean, posterior_sd) * 100, 1), \"%)\"), \n           color = \"blue\", size = 3, hjust = 0, fontface = \"bold\") +\n  # Add comprehensive explanations for each distribution\n  # POSTERIOR annotation (top left) - moved up more and left more\n  annotate(\"text\", x = posterior_x, y = posterior_y_start, \n           label = \"POSTERIOR (Blue):\", size = 3, fontface = \"bold\", hjust = 0, color = \"darkblue\") +\n  annotate(\"text\", x = posterior_x, y = posterior_y_start - 0.05, \n           label = \"Updated belief after data\", size = 2.5, hjust = 0, color = \"darkblue\") +\n  annotate(\"text\", x = posterior_x, y = posterior_y_start - 0.08, \n           label = paste0(\"Confidence in posterior mean = \", round(dnorm(posterior_mean, posterior_mean, posterior_sd) * 100, 1), \"%\"), size = 2.5, hjust = 0, color = \"darkblue\") +\n  annotate(\"text\", x = posterior_x, y = posterior_y_start - 0.11, \n           label = \"Combines prior + likelihood\", size = 2.5, hjust = 0, color = \"darkblue\") +\n  \n  annotate(\"text\", x = likelihood_x, y = likelihood_y_start, \n           label = \"LIKELIHOOD (Orange):\", size = 3, fontface = \"bold\", hjust = 0, color = \"darkorange\") +\n  annotate(\"text\", x = likelihood_x, y = likelihood_y_start - 0.05, \n           label = \"H0: No difference vs H1: Some difference\", size = 2.5, hjust = 0, color = \"darkorange\") +\n  annotate(\"text\", x = likelihood_x, y = likelihood_y_start - 0.08, \n           label = paste0(\"Observed difference = \", round(observed_diff, 3)), size = 2.5, hjust = 0, color = \"darkorange\") +\n  annotate(\"text\", x = likelihood_x, y = likelihood_y_start - 0.11, \n           label = paste0(\"Confidence in observed effect = \", round(dnorm(observed_diff, observed_diff, likelihood_sd) * 100, 1), \"%\"), size = 2.5, hjust = 0, color = \"darkorange\") +\n  \n  # PRIOR annotation (top right) - moved up more\n  annotate(\"text\", x = prior_x, y = prior_y_start, \n           label = \"PRIOR (Grey):\", size = 3, fontface = \"bold\", hjust = 0) +\n  annotate(\"text\", x = prior_x, y = prior_y_start - 0.05, \n           label = \"Skeptical belief before data\", size = 2.5, hjust = 0) +\n  annotate(\"text\", x = prior_x, y = prior_y_start - 0.08, \n           label = \"Centered at 0 (no effect)\", size = 2.5, hjust = 0) +\n  annotate(\"text\", x = prior_x, y = prior_y_start - 0.11, \n           label = \"Wide spread = uncertainty\", size = 2.5, hjust = 0) +\n  \n  # Add interpretation of the null hypothesis line\n  annotate(\"text\", x = null_x, y = null_y_start, \n           label = \"NULL HYPOTHESIS (Red):\", size = 3, fontface = \"bold\", hjust = 0, color = \"darkred\") +\n  annotate(\"text\", x = null_x, y = null_y_start - 0.05, \n           label = \"H0: No treatment effect\", size = 2.5, hjust = 0, color = \"darkred\") +\n  annotate(\"text\", x = null_x, y = null_y_start - 0.08, \n           label = \"Values &lt; 0 = Usual care better\", size = 2.5, hjust = 0, color = \"darkred\") +\n  annotate(\"text\", x = null_x, y = null_y_start - 0.11, \n           label = \"Values &gt; 0 = Chatbot better\", size = 2.5, hjust = 0, color = \"darkred\") +\n  \n  # Add natural language interpretation aligned with NULL HYPOTHESIS\n  annotate(\"text\", x = interpretation_x, y = interpretation_y_start, \n           label = \"INTERPRETATION:\", size = 3, fontface = \"bold\", hjust = 0, color = \"darkgreen\") +\n  annotate(\"text\", x = interpretation_x, y = interpretation_y_start - 0.05, \n           label = paste0(\"If I had 50% credence that there was no difference\"), size = 2.5, hjust = 0, color = \"darkgreen\") +\n  annotate(\"text\", x = interpretation_x, y = interpretation_y_start - 0.08, \n           label = paste0(\"before this evidence, after updating on ONLY this\"), size = 2.5, hjust = 0, color = \"darkgreen\") +\n  annotate(\"text\", x = interpretation_x, y = interpretation_y_start - 0.11, \n           label = paste0(\"I should now believe it with confidence of \", round(prob_positive * 100, 1), \"%\"), size = 2.5, hjust = 0, color = \"darkgreen\") +\n  annotate(\"text\", x = interpretation_x, y = interpretation_y_start - 0.14, \n           label = paste0(\"Prior confidence in effect size = \", round(dnorm(observed_diff, 0, 1) * 100, 1), \"%\"), size = 2.5, hjust = 0, color = \"darkgreen\") +\n  annotate(\"text\", x = interpretation_x, y = interpretation_y_start - 0.17, \n           label = paste0(\"After updating: confidence in effect size = \", round(dnorm(observed_diff, posterior_mean, posterior_sd) * 100, 1), \"%\"), size = 2.5, hjust = 0, color = \"darkgreen\") +\n  \n  labs(title = \"Bayesian Analysis: Prior, Likelihood, and Posterior\",\n       subtitle = paste(\"Confidence in posterior mean =\", round(dnorm(posterior_mean, posterior_mean, posterior_sd) * 100, 1), \"%\", \n                       \"| Posterior mean =\", round(posterior_mean, 3)),\n       x = \"Effect Size (Difference in PHQ-9 Change)\", y = \"Density\",\n       caption = \"Red line = H0 (no effect) | Blue line = Posterior mean | Gray = Prior | Orange = Likelihood | Blue = Posterior\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_color_manual(values = c(\"Prior\" = \"gray\", \"Likelihood\" = \"orange\", \"Posterior\" = \"blue\"))\n\nprint(bayesian_plot)\n\n\n\n\n\n\n\n\n\n# Low Cohen's d suggests the effect might be smaller than initially estimated\n# We'll use the observed Cohen's d as evidence to update our belief\nstage2_prior_mean &lt;- posterior_mean\nstage2_prior_sd &lt;- posterior_sd\nstage2_likelihood_mean &lt;- cohens_d  # Cohen's d as evidence\nstage2_likelihood_sd &lt;- 0.1  # Uncertainty around Cohen's d estimate\n\n# Stage 2 posterior parameters\nstage2_posterior_precision &lt;- 1/stage2_prior_sd^2 + 1/stage2_likelihood_sd^2\nstage2_posterior_sd &lt;- sqrt(1/stage2_posterior_precision)\nstage2_posterior_mean &lt;- (stage2_prior_mean/stage2_prior_sd^2 + stage2_likelihood_mean/stage2_likelihood_sd^2) / stage2_posterior_precision\n\n\nShow the code\n# Visualization 16: Comparison of significant vs non-significant outcomes\n# Calculate statistics for both measures\nphq9_change_chatbot &lt;- therapy_data$change_phq9[therapy_data$group == \"Chatbot\"]\nphq9_change_usual &lt;- therapy_data$change_phq9[therapy_data$group == \"Usual Care\"]\nepds_change_chatbot &lt;- therapy_data$change_epds[therapy_data$group == \"Chatbot\"]\nepds_change_usual &lt;- therapy_data$change_epds[therapy_data$group == \"Usual Care\"]\n\n# T-tests for both measures\nphq9_test &lt;- t.test(phq9_change_chatbot, phq9_change_usual)\nepds_test &lt;- t.test(epds_change_chatbot, epds_change_usual)\n\n# Effect sizes\nphq9_cohens_d &lt;- (mean(phq9_change_chatbot) - mean(phq9_change_usual)) / \n                 sqrt(((length(phq9_change_chatbot)-1)*var(phq9_change_chatbot) + \n                       (length(phq9_change_usual)-1)*var(phq9_change_usual)) / \n                      (length(phq9_change_chatbot) + length(phq9_change_usual) - 2))\n\nepds_cohens_d &lt;- (mean(epds_change_chatbot) - mean(epds_change_usual)) / \n                 sqrt(((length(epds_change_chatbot)-1)*var(epds_change_chatbot) + \n                       (length(epds_change_usual)-1)*var(epds_change_usual)) / \n                      (length(epds_change_chatbot) + length(epds_change_usual) - 2))\n\n# Create comparison data\ncomparison_data &lt;- data.frame(\n  measure = rep(c(\"PHQ-9\", \"EPDS\"), each = 2),\n  group = rep(c(\"Chatbot\", \"Usual Care\"), 2),\n  mean_change = c(mean(phq9_change_chatbot), mean(phq9_change_usual),\n                  mean(epds_change_chatbot), mean(epds_change_usual)),\n  se = c(sd(phq9_change_chatbot)/sqrt(length(phq9_change_chatbot)), \n         sd(phq9_change_usual)/sqrt(length(phq9_change_usual)),\n         sd(epds_change_chatbot)/sqrt(length(epds_change_chatbot)), \n         sd(epds_change_usual)/sqrt(length(epds_change_usual))),\n  p_value = c(phq9_test$p.value, phq9_test$p.value, epds_test$p.value, epds_test$p.value),\n  cohens_d = c(phq9_cohens_d, phq9_cohens_d, epds_cohens_d, epds_cohens_d)\n)\n\ncomparison_plot &lt;- ggplot(comparison_data, aes(x = measure, y = mean_change, fill = group)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.8) +\n  geom_errorbar(aes(ymin = mean_change - 1.96*se, ymax = mean_change + 1.96*se), \n                position = position_dodge(width = 0.9), width = 0.2, linewidth = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  geom_text(aes(label = paste0(\"p = \", round(p_value, 3), \"\\nd = \", round(cohens_d, 3))), \n            position = position_dodge(width = 0.9), vjust = -0.5, size = 3) +\n  labs(title = \"Comparison: Significant vs Non-Significant Outcomes\",\n       subtitle = \"PHQ-9 (significant) vs EPDS (non-significant)\",\n       x = \"Depression Measure\", y = \"Mean Change in Score\",\n       caption = add_color_samples(\"Error bars = 95% CI | Red line = No change | Negative = Improvement\", c(\"Red\" = \"red\"))) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_fill_manual(values = c(\"Chatbot\" = \"steelblue\", \"Usual Care\" = \"orange\"))\n\nprint(comparison_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 17: Side-by-side null hypothesis testing\n# Create null distributions for both measures\nset.seed(456)\nn_simulations &lt;- 10000\n\n# PHQ-9 null distribution\nphq9_null_differences &lt;- numeric(n_simulations)\nfor(i in 1:n_simulations) {\n  shuffled_phq9 &lt;- sample(therapy_data$change_phq9)\n  chatbot_phq9 &lt;- shuffled_phq9[1:length(phq9_change_chatbot)]\n  usual_phq9 &lt;- shuffled_phq9[(length(phq9_change_chatbot)+1):length(shuffled_phq9)]\n  phq9_null_differences[i] &lt;- mean(chatbot_phq9) - mean(usual_phq9)\n}\n\n# EPDS null distribution\nepds_null_differences &lt;- numeric(n_simulations)\nfor(i in 1:n_simulations) {\n  shuffled_epds &lt;- sample(therapy_data$change_epds)\n  chatbot_epds &lt;- shuffled_epds[1:length(epds_change_chatbot)]\n  usual_epds &lt;- shuffled_epds[(length(epds_change_chatbot)+1):length(shuffled_epds)]\n  epds_null_differences[i] &lt;- mean(chatbot_epds) - mean(usual_epds)\n}\n\n# Observed differences\nphq9_observed_diff &lt;- mean(phq9_change_chatbot) - mean(phq9_change_usual)\nepds_observed_diff &lt;- mean(epds_change_chatbot) - mean(epds_change_usual)\n\n# Create combined null distribution plot\nnull_comparison_data &lt;- data.frame(\n  difference = c(phq9_null_differences, epds_null_differences),\n  measure = rep(c(\"PHQ-9\", \"EPDS\"), each = n_simulations)\n)\n\nnull_comparison_plot &lt;- ggplot(null_comparison_data, aes(x = difference, fill = measure)) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\") +\n  geom_vline(xintercept = phq9_observed_diff, color = \"red\", linewidth = 2) +\n  geom_vline(xintercept = epds_observed_diff, color = \"blue\", linewidth = 2) +\n  geom_vline(xintercept = 0, color = \"black\", linewidth = 1, linetype = \"dotted\") +\n  facet_wrap(~measure, scales = \"free_x\") +\n  labs(title = \"Null Hypothesis Testing: PHQ-9 vs EPDS\",\n       subtitle = paste(\"PHQ-9 p =\", round(mean(abs(phq9_null_differences) &gt;= abs(phq9_observed_diff)), 4),\n                       \"| EPDS p =\", round(mean(abs(epds_null_differences) &gt;= abs(epds_observed_diff)), 4)),\n       x = \"Difference in Mean Change (Chatbot - Usual Care)\", y = \"Frequency\",\n       caption = add_color_samples(\"Red line = PHQ-9 observed | Blue line = EPDS observed | Black = H0\", \n                                 c(\"Red\" = \"red\", \"Blue\" = \"blue\"))) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_fill_manual(values = c(\"PHQ-9\" = \"red\", \"EPDS\" = \"blue\"))\n\nprint(null_comparison_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 18: Effect size comparison\neffect_size_comparison &lt;- data.frame(\n  measure = c(\"PHQ-9\", \"EPDS\"),\n  cohens_d = c(phq9_cohens_d, epds_cohens_d),\n  p_value = c(phq9_test$p.value, epds_test$p.value),\n  significant = c(phq9_test$p.value &lt; 0.05, epds_test$p.value &lt; 0.05)\n)\n\neffect_comparison_plot &lt;- ggplot(effect_size_comparison, aes(x = measure, y = abs(cohens_d), fill = significant)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  geom_text(aes(label = paste0(\"d = \", round(cohens_d, 3), \"\\np = \", round(p_value, 4))), \n            vjust = -0.5, size = 4) +\n  geom_hline(yintercept = c(0.2, 0.5, 0.8), linetype = \"dashed\", color = \"gray\", alpha = 0.7) +\n  geom_text(aes(x = 0.5, y = 0.25, label = \"Small\"), color = \"gray\", size = 3) +\n  geom_text(aes(x = 0.5, y = 0.55, label = \"Medium\"), color = \"gray\", size = 3) +\n  geom_text(aes(x = 0.5, y = 0.85, label = \"Large\"), color = \"gray\", size = 3) +\n  labs(title = \"Effect Size Comparison: PHQ-9 vs EPDS\",\n       subtitle = \"Absolute Cohen's d values with significance\",\n       x = \"Depression Measure\", y = \"|Cohen's d|\",\n       caption = \"Dashed lines = Effect size thresholds | Color = Statistical significance\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_fill_manual(values = c(\"TRUE\" = \"darkgreen\", \"FALSE\" = \"orange\"))\n\nprint(effect_comparison_plot)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Visualization 19: Confidence intervals comparison\n# Calculate CIs for both measures\nphq9_ci &lt;- t.test(phq9_change_chatbot, phq9_change_usual)$conf.int\nepds_ci &lt;- t.test(epds_change_chatbot, epds_change_usual)$conf.int\n\nci_comparison_data &lt;- data.frame(\n  measure = c(\"PHQ-9\", \"EPDS\"),\n  mean_diff = c(phq9_observed_diff, epds_observed_diff),\n  lower = c(phq9_ci[1], epds_ci[1]),\n  upper = c(phq9_ci[2], epds_ci[2]),\n  significant = c(phq9_ci[1] &gt; 0 || phq9_ci[2] &lt; 0, epds_ci[1] &gt; 0 || epds_ci[2] &lt; 0)\n)\n\nci_comparison_plot &lt;- ggplot(ci_comparison_data, aes(y = measure)) +\n  geom_errorbar(aes(xmin = lower, xmax = upper, color = significant), \n                linewidth = 2, width = 0.3) +\n  geom_point(aes(x = mean_diff, color = significant), size = 4) +\n  geom_vline(xintercept = 0, color = \"red\", linewidth = 2, linetype = \"dashed\") +\n  geom_text(aes(x = lower - 0.1, label = paste0(\"[\", round(lower, 3), \", \", round(upper, 3), \"]\")), \n            hjust = 1, size = 3) +\n  labs(title = \"95% Confidence Intervals: PHQ-9 vs EPDS\",\n       subtitle = \"Does the interval exclude zero?\",\n       x = \"Difference in Mean Change (Chatbot - Usual Care)\", y = \"Measure\",\n       caption = add_color_samples(\"Red line = H0 (no difference) | Color = CI excludes 0\", c(\"Red\" = \"red\"))) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_color_manual(values = c(\"TRUE\" = \"darkgreen\", \"FALSE\" = \"orange\"))\n\nprint(ci_comparison_plot)"
  },
  {
    "objectID": "concepts/degrees-of-freedom.html",
    "href": "concepts/degrees-of-freedom.html",
    "title": "Degrees of Freedom",
    "section": "",
    "text": "Degrees of freedom (df) is one of the most confusing concepts in statistics, yet it’s fundamental to understanding statistical inference."
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#introduction",
    "href": "concepts/degrees-of-freedom.html#introduction",
    "title": "Degrees of Freedom",
    "section": "",
    "text": "Degrees of freedom (df) is one of the most confusing concepts in statistics, yet it’s fundamental to understanding statistical inference."
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#what-are-degrees-of-freedom",
    "href": "concepts/degrees-of-freedom.html#what-are-degrees-of-freedom",
    "title": "Degrees of Freedom",
    "section": "What Are Degrees of Freedom?",
    "text": "What Are Degrees of Freedom?\nDegrees of freedom represent the number of independent pieces of information available to estimate a parameter or test a hypothesis. Think of it as the number of “free choices” you have after accounting for constraints."
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#vector-examples",
    "href": "concepts/degrees-of-freedom.html#vector-examples",
    "title": "Degrees of Freedom",
    "section": "Vector Examples",
    "text": "Vector Examples"
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#the-intuition-the-free-to-vary-concept",
    "href": "concepts/degrees-of-freedom.html#the-intuition-the-free-to-vary-concept",
    "title": "Degrees of Freedom",
    "section": "The Intuition: The “Free to Vary” Concept",
    "text": "The Intuition: The “Free to Vary” Concept\n\nSimple Example: Three Numbers That Sum to 10\nLet’s start with a simple example to build intuition:\n\n\nShow the code\n# Check and install required packages\nrequired_packages &lt;- c(\"ggplot2\", \"dplyr\")\n\n# Load required packages\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n# Install and load packages\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n\n\n\nShow the code\n# Example: Three numbers that sum to 10\ncat(\"Constraint: x + y + z = 10\\n\\n\")\n\n\nConstraint: x + y + z = 10\n\n\nShow the code\n# If we know two numbers, the third is determined\nx &lt;- 3\ny &lt;- 4\nz &lt;- 10 - x - y  # z is NOT free to vary\n\ncat(\"If x =\", x, \"and y =\", y, \"then z MUST be\", z, \"\\n\")\n\n\nIf x = 3 and y = 4 then z MUST be 3 \n\n\nShow the code\ncat(\"Degrees of freedom = 2 (only x and y are free to vary)\\n\")\n\n\nDegrees of freedom = 2 (only x and y are free to vary)\n\n\n\n\nVisualizing the Constraint\n\n\nShow the code\n# Create a 2D visualization of the constraint x + y + z = 10\n# We'll show how z is determined by x and y\n\n# Generate points on the constraint plane\nx_vals &lt;- seq(0, 10, length.out = 50)\ny_vals &lt;- seq(0, 10, length.out = 50)\n\n# Create grid of x and y values\ngrid_data &lt;- expand.grid(x = x_vals, y = y_vals)\ngrid_data$z &lt;- 10 - grid_data$x - grid_data$y\n\n# Filter valid points (all coordinates &gt;= 0)\nvalid_points &lt;- grid_data[grid_data$z &gt;= 0, ]\n\n# Create 2D plot showing the constraint\nggplot(valid_points, aes(x = x, y = y, color = z)) +\n  geom_point(size = 1, alpha = 0.6) +\n  scale_color_gradient(low = \"blue\", high = \"red\", name = \"z value\") +\n  labs(title = \"Constraint: x + y + z = 10\",\n       subtitle = \"Color shows the z value (red = high, blue = low)\",\n       x = \"x value\", y = \"y value\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  geom_abline(intercept = 10, slope = -1, color = \"black\", linewidth = 1, linetype = \"dashed\") +\n  annotate(\"text\", x = 8, y = 1, label = \"x + y = 10\", color = \"black\", size = 4)"
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#degrees-of-freedom-in-sample-variance",
    "href": "concepts/degrees-of-freedom.html#degrees-of-freedom-in-sample-variance",
    "title": "Degrees of Freedom",
    "section": "Degrees of Freedom in Sample Variance",
    "text": "Degrees of Freedom in Sample Variance\n\nThe Key Insight: Sample Mean Constrains the Data\nWhen we calculate sample variance, we use the sample mean as an estimate of the population mean. This creates a constraint that reduces our degrees of freedom.\n\n\nShow the code\n# Generate sample data\nsample_data &lt;- c(2, 4, 6, 8, 10)\nn &lt;- length(sample_data)\nsample_mean &lt;- mean(sample_data)\n\ncat(\"Sample data:\", paste(sample_data, collapse = \", \"), \"\\n\")\n\n\nSample data: 2, 4, 6, 8, 10 \n\n\nShow the code\ncat(\"Sample mean:\", sample_mean, \"\\n\")\n\n\nSample mean: 6 \n\n\nShow the code\ncat(\"Sample size (n):\", n, \"\\n\")\n\n\nSample size (n): 5 \n\n\nShow the code\ncat(\"Degrees of freedom for variance:\", n - 1, \"\\n\\n\")\n\n\nDegrees of freedom for variance: 4 \n\n\nShow the code\n# Show why we lose 1 degree of freedom\ncat(\"If we know the mean and n-1 values, the last value is determined:\\n\")\n\n\nIf we know the mean and n-1 values, the last value is determined:\n\n\nShow the code\nknown_values &lt;- sample_data[1:(n-1)]\nlast_value &lt;- n * sample_mean - sum(known_values)\ncat(\"Known values:\", paste(known_values, collapse = \", \"), \"\\n\")\n\n\nKnown values: 2, 4, 6, 8 \n\n\nShow the code\ncat(\"Last value MUST be:\", last_value, \"\\n\")\n\n\nLast value MUST be: 10 \n\n\nShow the code\ncat(\"This is why df = n - 1 for sample variance\\n\")\n\n\nThis is why df = n - 1 for sample variance\n\n\n\n\nVisualizing the Constraint in Sample Variance\n\n\nShow the code\n# Create visualization of the constraint\ndf_sample &lt;- data.frame(\n  x = 1:n,\n  y = sample_data,\n  point_type = \"Data\"\n)\n\n# Add the mean line\ndf_mean &lt;- data.frame(\n  x = c(0.5, n + 0.5),\n  y = c(sample_mean, sample_mean),\n  point_type = \"Mean\"\n)\n\n# Combine data\ndf_combined &lt;- rbind(df_sample, df_mean)\n\n# Create plot\nggplot(df_combined, aes(x = x, y = y, color = point_type)) +\n  geom_point(size = 3) +\n  geom_line(data = df_mean, linewidth = 2, linetype = \"dashed\") +\n  geom_segment(data = df_sample, aes(xend = x, yend = sample_mean), \n               color = \"red\", alpha = 0.5) +\n  labs(title = \"Sample Data with Mean Constraint\",\n       subtitle = \"Red lines show deviations from mean\",\n       x = \"Observation\", y = \"Value\",\n       color = \"Type\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_color_manual(values = c(\"Data\" = \"blue\", \"Mean\" = \"red\"))"
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#degrees-of-freedom-in-different-contexts",
    "href": "concepts/degrees-of-freedom.html#degrees-of-freedom-in-different-contexts",
    "title": "Degrees of Freedom",
    "section": "Degrees of Freedom in Different Contexts",
    "text": "Degrees of Freedom in Different Contexts\n\n1. One-Sample t-test\n\n\nShow the code\n# Simulate one-sample t-test\npopulation_mean &lt;- 100\nsample_size &lt;- 15\nsample_data &lt;- rnorm(sample_size, mean = 105, sd = 10)\n\n# Calculate t-statistic\nsample_mean &lt;- mean(sample_data)\nsample_se &lt;- sd(sample_data) / sqrt(sample_size)\nt_stat &lt;- (sample_mean - population_mean) / sample_se\ndf_t &lt;- sample_size - 1\n\ncat(\"One-Sample t-test:\\n\")\n\n\nOne-Sample t-test:\n\n\nShow the code\ncat(\"Sample size:\", sample_size, \"\\n\")\n\n\nSample size: 15 \n\n\nShow the code\ncat(\"Degrees of freedom:\", df_t, \"\\n\")\n\n\nDegrees of freedom: 14 \n\n\nShow the code\ncat(\"t-statistic:\", round(t_stat, 3), \"\\n\")\n\n\nt-statistic: 2.989 \n\n\nShow the code\ncat(\"p-value:\", round(2 * pt(-abs(t_stat), df_t), 4), \"\\n\")\n\n\np-value: 0.0098 \n\n\n\n\n2. Two-Sample t-test\n\n\nShow the code\n# Simulate two-sample t-test\nn1 &lt;- 12\nn2 &lt;- 10\ngroup1 &lt;- rnorm(n1, mean = 100, sd = 8)\ngroup2 &lt;- rnorm(n2, mean = 105, sd = 8)\n\n# Calculate pooled t-test\nmean1 &lt;- mean(group1)\nmean2 &lt;- mean(group2)\nvar1 &lt;- var(group1)\nvar2 &lt;- var(group2)\n\n# Pooled variance\npooled_var &lt;- ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\npooled_se &lt;- sqrt(pooled_var * (1/n1 + 1/n2))\nt_stat_pooled &lt;- (mean1 - mean2) / pooled_se\ndf_pooled &lt;- n1 + n2 - 2\n\ncat(\"Two-Sample t-test (pooled):\\n\")\n\n\nTwo-Sample t-test (pooled):\n\n\nShow the code\ncat(\"Group 1 size:\", n1, \"\\n\")\n\n\nGroup 1 size: 12 \n\n\nShow the code\ncat(\"Group 2 size:\", n2, \"\\n\")\n\n\nGroup 2 size: 10 \n\n\nShow the code\ncat(\"Degrees of freedom:\", df_pooled, \"\\n\")\n\n\nDegrees of freedom: 20 \n\n\nShow the code\ncat(\"t-statistic:\", round(t_stat_pooled, 3), \"\\n\")\n\n\nt-statistic: -3.414 \n\n\nShow the code\ncat(\"p-value:\", round(2 * pt(-abs(t_stat_pooled), df_pooled), 4), \"\\n\")\n\n\np-value: 0.0028 \n\n\n\n\nVisualizing t-Distributions with Different Degrees of Freedom\n\n\nShow the code\n# Create t-distributions with different degrees of freedom\nx_vals &lt;- seq(-4, 4, length.out = 200)\ndf_values &lt;- c(1, 3, 10, 30)\n\n# Calculate t-distribution densities\nt_densities &lt;- data.frame()\nfor(df in df_values) {\n  density_vals &lt;- dt(x_vals, df = df)\n  t_densities &lt;- rbind(t_densities, \n                       data.frame(x = x_vals, density = density_vals, df = paste(\"df =\", df)))\n}\n\n# Add normal distribution for comparison\nnormal_density &lt;- dnorm(x_vals, mean = 0, sd = 1)\nt_densities &lt;- rbind(t_densities, \n                     data.frame(x = x_vals, density = normal_density, df = \"Normal\"))\n\n# Create plot\nggplot(t_densities, aes(x = x, y = density, color = df)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"t-Distributions with Different Degrees of Freedom\",\n       subtitle = \"As df increases, t-distribution approaches normal\",\n       x = \"Value\", y = \"Density\",\n       color = \"Distribution\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_color_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#degrees-of-freedom-in-chi-square-tests",
    "href": "concepts/degrees-of-freedom.html#degrees-of-freedom-in-chi-square-tests",
    "title": "Degrees of Freedom",
    "section": "Degrees of Freedom in Chi-Square Tests",
    "text": "Degrees of Freedom in Chi-Square Tests\n\nChi-Square Goodness of Fit\n\n\nShow the code\n# Simulate chi-square goodness of fit test\nobserved &lt;- c(22, 18, 25, 15, 20)  # Observed counts\nexpected &lt;- rep(20, 5)  # Expected counts (equal probability)\n\n# Calculate chi-square statistic\nchi_sq &lt;- sum((observed - expected)^2 / expected)\ndf_chi &lt;- length(observed) - 1  # k - 1 categories\n\ncat(\"Chi-Square Goodness of Fit Test:\\n\")\n\n\nChi-Square Goodness of Fit Test:\n\n\nShow the code\ncat(\"Number of categories:\", length(observed), \"\\n\")\n\n\nNumber of categories: 5 \n\n\nShow the code\ncat(\"Degrees of freedom:\", df_chi, \"\\n\")\n\n\nDegrees of freedom: 4 \n\n\nShow the code\ncat(\"Chi-square statistic:\", round(chi_sq, 3), \"\\n\")\n\n\nChi-square statistic: 2.9 \n\n\nShow the code\ncat(\"p-value:\", round(1 - pchisq(chi_sq, df_chi), 4), \"\\n\")\n\n\np-value: 0.5747 \n\n\n\n\nVisualizing Chi-Square Distributions\n\n\nShow the code\n# Create chi-square distributions with different degrees of freedom\nx_vals &lt;- seq(0, 20, length.out = 200)\ndf_values_chi &lt;- c(1, 2, 5, 10)\n\n# Calculate chi-square distribution densities\nchi_densities &lt;- data.frame()\nfor(df in df_values_chi) {\n  density_vals &lt;- dchisq(x_vals, df = df)\n  chi_densities &lt;- rbind(chi_densities, \n                         data.frame(x = x_vals, density = density_vals, df = paste(\"df =\", df)))\n}\n\n# Create plot\nggplot(chi_densities, aes(x = x, y = density, color = df)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Chi-Square Distributions with Different Degrees of Freedom\",\n       subtitle = \"Shape becomes more symmetric as df increases\",\n       x = \"Value\", y = \"Density\",\n       color = \"Degrees of Freedom\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  scale_color_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#degrees-of-freedom-in-regression",
    "href": "concepts/degrees-of-freedom.html#degrees-of-freedom-in-regression",
    "title": "Degrees of Freedom",
    "section": "Degrees of Freedom in Regression",
    "text": "Degrees of Freedom in Regression\n\nSimple Linear Regression\n\n\nShow the code\n# Generate data for simple linear regression\nn_reg &lt;- 20\nx_reg &lt;- seq(1, 10, length.out = n_reg)\ny_reg &lt;- 2 + 1.5 * x_reg + rnorm(n_reg, 0, 2)\n\n# Fit regression model\nlm_model &lt;- lm(y_reg ~ x_reg)\nresiduals &lt;- residuals(lm_model)\n\n# Degrees of freedom\ndf_total &lt;- n_reg - 1      # Total df\ndf_model &lt;- 1              # Model df (one slope parameter)\ndf_residual &lt;- n_reg - 2   # Residual df\n\ncat(\"Simple Linear Regression:\\n\")\n\n\nSimple Linear Regression:\n\n\nShow the code\ncat(\"Sample size:\", n_reg, \"\\n\")\n\n\nSample size: 20 \n\n\nShow the code\ncat(\"Total df:\", df_total, \"\\n\")\n\n\nTotal df: 19 \n\n\nShow the code\ncat(\"Model df:\", df_model, \"\\n\")\n\n\nModel df: 1 \n\n\nShow the code\ncat(\"Residual df:\", df_residual, \"\\n\")\n\n\nResidual df: 18 \n\n\nShow the code\ncat(\"R-squared:\", round(summary(lm_model)$r.squared, 3), \"\\n\")\n\n\nR-squared: 0.848 \n\n\n\n\nVisualizing Regression Degrees of Freedom\n\n\nShow the code\n# Create regression plot\ndf_reg &lt;- data.frame(x = x_reg, y = y_reg)\n\nggplot(df_reg, aes(x = x, y = y)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", color = \"red\", linewidth = 1) +\n  geom_segment(aes(xend = x, yend = fitted(lm_model)), \n               color = \"green\", alpha = 0.5) +\n  labs(title = \"Simple Linear Regression\",\n       subtitle = \"Green lines show residuals (constrained by regression line)\",\n       x = \"X\", y = \"Y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))"
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#interactive-example-degrees-of-freedom-calculator",
    "href": "concepts/degrees-of-freedom.html#interactive-example-degrees-of-freedom-calculator",
    "title": "Degrees of Freedom",
    "section": "Interactive Example: Degrees of Freedom Calculator",
    "text": "Interactive Example: Degrees of Freedom Calculator\n\n\nShow the code\n# Function to demonstrate degrees of freedom\ndemonstrate_df &lt;- function(n, constraint_type = \"mean\") {\n  \n  if(constraint_type == \"mean\") {\n    # Generate n random numbers\n    data &lt;- rnorm(n, mean = 10, sd = 2)\n    \n    # Show how the last value is constrained\n    if(n &gt; 1) {\n      known_values &lt;- data[1:(n-1)]\n      target_mean &lt;- mean(data)\n      last_value &lt;- n * target_mean - sum(known_values)\n      \n      cat(\"Degrees of Freedom Demonstration:\\n\")\n      cat(\"Sample size:\", n, \"\\n\")\n      cat(\"Constraint: Sample mean =\", round(target_mean, 2), \"\\n\")\n      cat(\"Known values:\", paste(round(known_values, 2), collapse = \", \"), \"\\n\")\n      cat(\"Last value MUST be:\", round(last_value, 2), \"\\n\")\n      cat(\"Degrees of freedom =\", n - 1, \"\\n\")\n    }\n  }\n}\n\n# Demonstrate with different sample sizes\ndemonstrate_df(5, \"mean\")\n\n\nDegrees of Freedom Demonstration:\nSample size: 5 \nConstraint: Sample mean = 10.32 \nKnown values: 11.17, 10.25, 10.43, 10.76 \nLast value MUST be: 9 \nDegrees of freedom = 4 \n\n\nShow the code\ncat(\"\\n\")\n\n\nShow the code\ndemonstrate_df(10, \"mean\")\n\n\nDegrees of Freedom Demonstration:\nSample size: 10 \nConstraint: Sample mean = 9.71 \nKnown values: 9.33, 7.96, 7.86, 10.61, 10.9, 10.11, 11.84, 14.1, 9.02 \nLast value MUST be: 5.38 \nDegrees of freedom = 9"
  },
  {
    "objectID": "concepts/degrees-of-freedom.html#key-takeaways",
    "href": "concepts/degrees-of-freedom.html#key-takeaways",
    "title": "Degrees of Freedom",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nWhy Degrees of Freedom Matter:\n\nCorrects for bias: Using n-1 instead of n in sample variance gives an unbiased estimate\nAccounts for constraints: Each constraint reduces the number of independent pieces of information\nAffects statistical tests: Different degrees of freedom lead to different critical values\nDetermines distribution shape: Higher degrees of freedom make distributions more normal\n\n\n\nCommon Rules:\n\nSample variance: df = n - 1\nOne-sample t-test: df = n - 1\nTwo-sample t-test: df = n₁ + n₂ - 2\nChi-square goodness of fit: df = k - 1 (k categories)\nSimple linear regression: df_residual = n - 2\n\n\n\nThe Intuition:\nThink of degrees of freedom as the number of “free choices” you have after accounting for the constraints imposed by your statistical procedure. Each constraint reduces your degrees of freedom by one."
  }
]