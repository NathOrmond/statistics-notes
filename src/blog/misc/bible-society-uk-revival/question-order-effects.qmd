---
title: "Question Order Effects"
subtitle: "Impact of survey design differences on responses"
execute:
  echo: true
  warning: false
  message: false
---

```{r}
#| label: setup
#| include: false

# Set working directory to project root using here package
if (!require(here, quietly = TRUE)) {
  install.packages("here")
}
# Find project root (directory containing _quarto.yml)
here::i_am("_quarto.yml")
# Set working directory to project root

library(tidyverse)
library(scales)
library(ggplot2)
```

## The Problem

The 2018 and 2024 surveys used fundamentally different question formats:

- **2018**: Single frequency question only
- **2024**: Binary question FIRST ("Yes - in the past year"), THEN frequency question

This violates best practice in survey methodology, as question order can significantly bias responses.

## Visualizing the Structural Difference

The most significant methodological flaw is the change in question structure between the two surveys. The 2018 survey asked a single question about frequency, while the 2024 survey first asked a binary yes/no question before asking about frequency. This change introduces a "priming" effect that makes the results from the two years not directly comparable. The chart below illustrates this structural difference.

```{r}
#| label: question-flow-chart
#| fig-cap: "Visual comparison of the 2018 and 2024 survey question structures."
#| echo: false

library(ggplot2)
library(tibble)
library(dplyr)

nodes <- tribble(
  ~survey, ~step, ~label,
  "2018 Survey", 1, "1. How often do you\nattend church?",
  "2024 Survey", 1, "1. Have you attended church\nin the past year?",
  "2024 Survey", 2, "2. How often did you attend?"
) %>% 
  mutate(x = 1, y = -step)

edges <- tribble(
  ~survey, ~x, ~y, ~xend, ~yend,
  "2024 Survey", 1, -1.2, 1, -1.8
)

ggplot() +
  geom_label(data = nodes, aes(x = x, y = y, label = label, fill = survey),
             color = "white", fontface = "bold", size = 4, lineheight = 0.9, 
             label.padding = unit(0.5, "lines"), label.size = 0) +
  geom_segment(data = edges, aes(x=x,y=y,xend=xend,yend=yend),
               arrow = arrow(length = unit(0.3, "cm")), linewidth = 1.5, color = "grey30") +
  facet_wrap(~survey, ncol = 2) +
  scale_fill_manual(values = c("2018 Survey" = "#4575b4", "2024 Survey" = "#d73027")) +
  theme_void() +
  labs(
    title = "Comparability Assessment: Survey Question Structure",
    subtitle = "The 2024 survey introduced a binary question, priming respondents before the frequency question."
  ) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 11, hjust = 0.5, color = "grey40"),
    strip.text = element_text(size = 14, face = "bold"),
    legend.position = "none",
    plot.margin = margin(10,10,10,10)
  )
```

üö© **CRITICAL**: The binary question primes respondents before they consider frequency, which can lead to:

- Acquiescence bias (tendency to say 'yes')
- Anchoring effects (binary framing affects frequency judgements)
- Non-comparable measures between years

## Internal Consistency Check

If the questions are measuring the same thing, the binary response should match the sum of frequency responses:

```{r}
#| label: consistency-check

attendance_data <- read_csv(here::here("data/bible-society-uk-revival/processed/church-attendance-extracted.csv"))

# 2024 binary response
binary_2024 <- attendance_data %>%
  filter(year == 2024, question_type == "binary", 
         response_category == "Yes - in the past year") %>%
  pull(total_pct)

# Sum of frequency responses (past year categories)
freq_2024 <- attendance_data %>%
  filter(year == 2024, question_type == "frequency") %>%
  filter(response_category %in% c(
    "Daily/almost daily", "A few times a week", 
    "About once a week", "About once a fortnight",
    "About once a month", "A few times a year",
    "About once a year"
  ))

freq_sum_2024 <- sum(freq_2024$total_pct, na.rm = TRUE)
discrepancy <- abs(freq_sum_2024 - binary_2024)
is_inconsistent <- discrepancy > 2
```

### Internal Consistency Check (2024)

If the questions are measuring the same thing, the binary response should match the sum of frequency responses:

- **Binary 'Yes - in the past year'**: `r sprintf("%.1f", binary_2024)`%
- **Sum of frequency categories**: `r sprintf("%.1f", freq_sum_2024)`%
- **Discrepancy**: `r sprintf("%.1f", discrepancy)` percentage points

`r if (is_inconsistent) {
  "üö© **INCONSISTENCY DETECTED**\n\nThis discrepancy suggests:\n1. Question order effects are influencing responses\n2. Respondents interpret the questions differently\n3. Measurement error in the data extraction process"
} else {
  "‚úì Responses are internally consistent"
}`

```{r}
#| label: inconsistency-chart
#| fig-cap: "Internal inconsistency in the 2024 survey. The sum of frequency responses does not match the binary 'Yes' response."
#| echo: false

inconsistency_data <- tibble(
  Source = c("Binary Question\\n('Yes - in past year')", "Sum of Frequency Questions\\n(annual categories)"),
  Percentage = c(binary_2024, freq_sum_2024)
)

ggplot(inconsistency_data, aes(x = Source, y = Percentage, fill = Source)) +
  geom_col(width = 0.6, alpha = 0.8) +
  geom_text(aes(label = sprintf("%.1f%%", Percentage)), vjust = -0.5, size = 5, fontface = "bold") +
  scale_fill_manual(values = c("#d7191c", "#fdae61")) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Internal Inconsistency in 2024 Survey Data",
    subtitle = sprintf("Discrepancy of %.1f percentage points between two measures of annual attendance", discrepancy),
    x = "Data Source within 2024 Survey",
    y = "Percentage (%)"
  ) +
  theme(
    legend.position = "none",
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "grey40"),
    panel.grid.major.x = element_blank()
  ) +
  ylim(0, 30)
```

## Evidence from Survey Methodology Literature

Question order effects are well-documented:

- **Schuman & Presser (1996)**: Found order effects of 5-15 percentage points
- **Acquiescence bias**: Tendency to agree with yes/no questions
- **Anchoring effects**: First question frames subsequent responses

The 2024 survey design violates the principle that questions should be asked in a way that minimises priming effects when making comparisons over time.

## Impact on Weekly Attendance Claim

```{r}
#| label: impact-analysis

# Weekly attendance in 2024 might be inflated by:
# 1. Acquiescence bias from binary question
# 2. Anchoring effects affecting frequency judgements

weekly_2018 <- attendance_data %>%
  filter(year == 2018, response_category == "At least once a week") %>%
  pull(total_pct) / 100

weekly_2024 <- attendance_data %>%
  filter(
    year == 2024,
    question_type == "frequency",
    response_category %in% c("Daily/almost daily", "A few times a week", "About once a week")
  ) %>%
  summarise(total_pct = sum(total_pct)) %>%
  pull(total_pct) / 100
```

### Weekly Attendance Comparison

The weekly attendance in 2024 might be inflated by acquiescence bias from the binary question and anchoring effects affecting frequency judgements:

- **2018 (frequency question only)**: `r sprintf("%.1f", weekly_2018 * 100)`%
- **2024 (after binary question)**: `r sprintf("%.1f", weekly_2024 * 100)`%
- **Difference**: `r sprintf("%+.1f", (weekly_2024 - weekly_2018) * 100)` percentage points

‚ö†Ô∏è **CAUTION**: The apparent increase may be an artefact of question order effects rather than true behavioural change. Without the binary priming question, the 2024 figure might be lower, showing no real change or even a decrease.

## The Contradictory Evidence

Interestingly, the binary question itself shows a **decrease**:

```{r}
#| label: contradictory-evidence

# 2018 "ever attended" estimate (sum of all frequency categories except "Never")
ever_2018 <- attendance_data %>%
  filter(year == 2018) %>%
  filter(response_category != "Never") %>%
  summarise(total = sum(total_pct, na.rm = TRUE)) %>%
  pull(total)

binary_2024 <- attendance_data %>%
  filter(year == 2024, question_type == "binary", 
         response_category == "Yes - in the past year") %>%
  pull(total_pct)
```

### 'Ever Attended' Comparison

Interestingly, the binary question itself shows a decrease:

- **2018 (frequency-based)**: `r sprintf("%.0f", ever_2018)`%
- **2024 (binary question)**: `r sprintf("%.1f", binary_2024)`%
- **Change**: `r sprintf("%.1f", binary_2024 - ever_2018)` percentage points

```{r}
#| label: contradictory-trend-chart
#| fig-cap: "Contradictory trends between weekly and annual attendance measures."
#| echo: false

trend_data <- tibble(
  Metric = c("Weekly Attendance", "Weekly Attendance", "Annual Attendance", "Annual Attendance"),
  Year = c("2018", "2024", "2018", "2024"),
  Value = c(weekly_2018 * 100, weekly_2024 * 100, ever_2018, binary_2024)
)

ggplot(trend_data, aes(x = Year, y = Value, group = Metric, color = Metric, shape = Metric)) +
  geom_line(linewidth = 2, alpha = 0.8) +
  geom_point(size = 5) +
  geom_text(aes(label = sprintf("%.1f%%", Value)),
            vjust = -1.5, fontface = "bold", size = 5) +
  scale_color_manual(values = c("Weekly Attendance" = "#2166ac", "Annual Attendance" = "#d73027")) +
  scale_shape_manual(values = c("Weekly Attendance" = 16, "Annual Attendance" = 15)) +
  guides(color = guide_legend(override.aes = list(linetype = 0))) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Contradictory Trends: Weekly vs. Annual Attendance",
    subtitle = "Weekly attendance appears to increase while annual attendance decreases‚Äîa logical impossibility.",
    x = "Survey Year",
    y = "Percentage (%)"
  ) +
  theme(
    legend.position = "top",
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "grey40")
  ) +
  ylim(0, 30)
```

üö© **CONTRADICTION**:

The binary question shows a DECREASE (27% ‚Üí 24%), while the frequency question shows an INCREASE (7% ‚Üí 11%). This contradiction is critical evidence:

These cannot both be true. This is strong evidence that question order effects are creating measurement error.

## Conclusion

The different question formats make direct comparison between 2018 and 2024 problematic. The question order effects likely:

1. Inflate the weekly attendance figure in 2024 due to acquiescence bias
2. Create internal inconsistencies within the 2024 survey
3. Produce contradictory results that cannot both be true

**Recommendation**: The 7% ‚Üí 11% increase claim should be treated with extreme caution. The methodological differences make the comparison invalid without controlling for question order effects.

